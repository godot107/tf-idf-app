{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Google Colab\n",
        "\n",
        "!git clone https://github.com/godot107/tf-idf-app.git\n",
        "%cd tf-idf-app\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install scattertext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IgYH2ALxin6",
        "outputId": "2121aeb1-e2d0-4684-8693-0fa2fde1a889",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tf-idf-app'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 27 (delta 9), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 162.04 KiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n",
            "/content/tf-idf-app\n",
            "Collecting en_core_web_sm@ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85 (from -r requirements.txt (line 6))\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Collecting spacy==3.8.3 (from -r requirements.txt (line 3))\n",
            "  Downloading spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: spacy-legacy==3.0.12 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers==1.0.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.0.5)\n",
            "Collecting scattertext (from -r requirements.txt (line 7))\n",
            "  Downloading scattertext-0.1.19-py3-none-any.whl.metadata (530 bytes)\n",
            "Collecting pandas==2.2.3 (from -r requirements.txt (line 8))\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (7.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==5.5.0->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair==5.5.0->-r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair==5.5.0->-r requirements.txt (line 1)) (1.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair==5.5.0->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair==5.5.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (3.0.9)\n",
            "Collecting thinc<8.4.0,>=8.3.0 (from spacy==3.8.3->-r requirements.txt (line 3))\n",
            "  Downloading thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (2.10.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.8.3->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.3->-r requirements.txt (line 8)) (2024.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scattertext->-r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scattertext->-r requirements.txt (line 7)) (1.6.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scattertext->-r requirements.txt (line 7)) (0.14.4)\n",
            "Collecting flashtext (from scattertext->-r requirements.txt (line 7))\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from scattertext->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->-r requirements.txt (line 9)) (3.0.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->scattertext->-r requirements.txt (line 7)) (7.1.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 9)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets->-r requirements.txt (line 9)) (6.3.3)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.5.0->-r requirements.txt (line 1)) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.5.0->-r requirements.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.5.0->-r requirements.txt (line 1)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair==5.5.0->-r requirements.txt (line 1)) (0.22.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.8.3->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.3->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.8.3->-r requirements.txt (line 3)) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.3->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.3->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.3->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.8.3->-r requirements.txt (line 3)) (2024.12.14)\n",
            "Collecting blis<1.2.0,>=1.1.0 (from thinc<8.4.0,>=8.3.0->spacy==3.8.3->-r requirements.txt (line 3))\n",
            "  Downloading blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.3->-r requirements.txt (line 3)) (0.20.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (6.5.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair==5.5.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scattertext->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.8.4)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.8.3->-r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (7.16.5)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.1.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 9)) (0.2.13)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->scattertext->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy==3.8.3->-r requirements.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (4.12.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (2.21.1)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->-r requirements.txt (line 9)) (1.2.2)\n",
            "Downloading spacy-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.1/29.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scattertext-0.1.19-py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9298 sha256=aa9f354149fe21224dd9fbb36a20db39283993d0dbb1e65ab941064e478dd50b\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext, en_core_web_sm, jedi, blis, pandas, thinc, spacy, scattertext\n",
            "  Attempting uninstall: en_core_web_sm\n",
            "    Found existing installation: en-core-web-sm 3.7.1\n",
            "    Uninstalling en-core-web-sm-3.7.1:\n",
            "      Successfully uninstalled en-core-web-sm-3.7.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.11\n",
            "    Uninstalling blis-0.7.11:\n",
            "      Successfully uninstalled blis-0.7.11\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.2.5\n",
            "    Uninstalling thinc-8.2.5:\n",
            "      Successfully uninstalled thinc-8.2.5\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.7.5\n",
            "    Uninstalling spacy-3.7.5:\n",
            "      Successfully uninstalled spacy-3.7.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-1.1.0 en_core_web_sm-3.8.0 flashtext-2.7 jedi-0.19.2 pandas-2.2.3 scattertext-0.1.19 spacy-3.8.3 thinc-8.3.3\n",
            "Requirement already satisfied: scattertext in /usr/local/lib/python3.10/dist-packages (0.1.19)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scattertext) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scattertext) (2.2.3)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from scattertext) (0.14.4)\n",
            "Requirement already satisfied: flashtext in /usr/local/lib/python3.10/dist-packages (from scattertext) (2.7)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from scattertext) (4.3.3)\n",
            "Requirement already satisfied: spacy>=3.2 in /usr/local/lib/python3.10/dist-packages (from scattertext) (3.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scattertext) (4.67.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->scattertext) (7.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (8.3.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (2.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2->scattertext) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->scattertext) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scattertext) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->scattertext) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scattertext) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels->scattertext) (1.0.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.2->scattertext) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2->scattertext) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->scattertext) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2->scattertext) (2024.12.14)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->scattertext) (1.17.0)\n",
            "Requirement already satisfied: blis<1.2.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.2->scattertext) (1.1.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.4.0,>=8.3.0->spacy>=3.2->scattertext) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.2->scattertext) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2->scattertext) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.2->scattertext) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2->scattertext) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import lemmatize, drawTilebars, dataminingdf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "A1A9cIa4xZd0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3P6JpjlIw_Tp",
        "outputId": "f89f7317-e6df-4742-a52c-3e95849347f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   docid         title                                               text  \\\n",
              "0      0        MATLAB   matlab ( matrix laboratory ) be a multi - par...   \n",
              "1      0        MATLAB   although matlab be intend primarily for numer...   \n",
              "2      0        MATLAB   as of 2018 , matlab have more than 3 million ...   \n",
              "3      1  Ray Kurzweil   raymond kurzweil ( ; bear february 12 , 1948 ...   \n",
              "4      1  Ray Kurzweil   kurzweil receive the 1999 national medal of t...   \n",
              "\n",
              "   tokencount                                   category  lineid  \n",
              "0          64  Data mining and machine learning software       0  \n",
              "1          48  Data mining and machine learning software       1  \n",
              "2          27  Data mining and machine learning software       2  \n",
              "3         105               Machine learning researchers       0  \n",
              "4         141               Machine learning researchers       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-874c20b2-0b27-4f23-a077-82238953f012\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>docid</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>tokencount</th>\n",
              "      <th>category</th>\n",
              "      <th>lineid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MATLAB</td>\n",
              "      <td>matlab ( matrix laboratory ) be a multi - par...</td>\n",
              "      <td>64</td>\n",
              "      <td>Data mining and machine learning software</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>MATLAB</td>\n",
              "      <td>although matlab be intend primarily for numer...</td>\n",
              "      <td>48</td>\n",
              "      <td>Data mining and machine learning software</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>MATLAB</td>\n",
              "      <td>as of 2018 , matlab have more than 3 million ...</td>\n",
              "      <td>27</td>\n",
              "      <td>Data mining and machine learning software</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Ray Kurzweil</td>\n",
              "      <td>raymond kurzweil ( ; bear february 12 , 1948 ...</td>\n",
              "      <td>105</td>\n",
              "      <td>Machine learning researchers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Ray Kurzweil</td>\n",
              "      <td>kurzweil receive the 1999 national medal of t...</td>\n",
              "      <td>141</td>\n",
              "      <td>Machine learning researchers</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-874c20b2-0b27-4f23-a077-82238953f012')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-874c20b2-0b27-4f23-a077-82238953f012 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-874c20b2-0b27-4f23-a077-82238953f012');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-47d64811-5720-44e3-9a9c-c02720f51176\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47d64811-5720-44e3-9a9c-c02720f51176')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-47d64811-5720-44e3-9a9c-c02720f51176 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataminingdf",
              "summary": "{\n  \"name\": \"dataminingdf\",\n  \"rows\": 855,\n  \"fields\": [\n    {\n      \"column\": \"docid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 132,\n        \"min\": 0,\n        \"max\": 453,\n        \"num_unique_values\": 453,\n        \"samples\": [\n          353,\n          39,\n          362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 453,\n        \"samples\": [\n          \"Sparse dictionary learning\",\n          \"Stochastic gradient descent\",\n          \"Darkforest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 855,\n        \"samples\": [\n          \" cursed phenomenon occur in domain such as numerical analysis , sampling , combinatoric , machine learning , datum mining and database . the common theme of these problem be that when the dimensionality increase , the volume of the space increase so fast that the available datum become sparse . this sparsity be problematic for any method that require statistical significance . in order to obtain a statistically sound and reliable result , the amount of datum need to support the result often grow exponentially with the dimensionality . also , organizing and search datum often rely on detect area where object form group with similar property ; in high dimensional datum , however , all object appear to be sparse and dissimilar in many way , which prevent common datum organization strategy from be efficient . \",\n          \" He receive the 2013 ieee icdm research contributions award for his research on datum mining algorithm such as dbscan , optics , local outlier factor and his work on mine high - dimensional datum . \",\n          \" the ugly duckling theorem be an argument show that classification be not really possible without some sort of bias . more particularly , it assume finitely many property combinable by logical connective , and finitely many object ; it assert that any two different object share the same number of ( extensional ) property . the theorem be name after hans christian andersen 's story \\\" the ugly duckling \\\" , because it show that a duckling be just as similar to a swan as two duckling be to each other . It be propose by satosi watanabe in 1969 . \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokencount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 11,\n        \"max\": 526,\n        \"num_unique_values\": 194,\n        \"samples\": [\n          435,\n          60,\n          178\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"Data mining algorithms\",\n          \"Machine learning researchers\",\n          \"Data analysis software\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lineid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 12,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          11,\n          9,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Sample documents with texts, just first 3 lines.\n",
        "# information retrieval app\n",
        "dataminingdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawTilebars(\"data\",normalized=True,sortby='title').display()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uBRvpHd4bHwc",
        "outputId": "7927ace1-ebde-401b-e4ed-89c39d2d43d9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the lemmatized query terms are:  ['datum']\n",
            "nomalized is  True\n",
            "I will sort by title\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6.vega-embed details,\n",
              "  #altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-1a105cbb3e0644c882b2ae5bcdc3ccc6\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-d27b280609f1e9b3824c9704554efba9\"}, \"facet\": {\"field\": \"title\", \"header\": {\"labelAnchor\": \"middle\", \"labelAngle\": 0, \"labelBaseline\": \"top\", \"labelOrient\": \"right\", \"labelPadding\": 10}, \"sort\": null, \"title\": null, \"type\": \"nominal\"}, \"spec\": {\"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"count\", \"legend\": null, \"scale\": {\"scheme\": \"blues\"}, \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"term\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}, {\"field\": \"lineid\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"lineid\", \"scale\": {\"domain\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}, \"title\": \"\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"term\", \"title\": \"\", \"type\": \"nominal\"}}}, \"columns\": 1, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-d27b280609f1e9b3824c9704554efba9\": [{\"docid\": 0, \"title\": \"MATLAB\", \"text\": \" matlab ( matrix laboratory ) be a multi - paradigm numerical computing environment and proprietary programming language develop by mathworks . matlab allow matrix manipulation , plot of function and datum , implementation of algorithm , creation of user interface , and interfac with program write in other language , include c , c++ , c # , java , fortran and python . \", \"tokencount\": 64, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 0, \"title\": \"MATLAB\", \"text\": \" although matlab be intend primarily for numerical computing , an optional toolbox use the mupad symbolic engine , allow access to symbolic computing ability . an additional package , simulink , add graphical multi - domain simulation and model - base design for dynamic and embed system . \", \"tokencount\": 48, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 0, \"title\": \"MATLAB\", \"text\": \" as of 2018 , matlab have more than 3 million user worldwide . matlab user come from various background of engineering , science , and economic . \", \"tokencount\": 27, \"category\": \"Data mining and machine learning software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 3, \"title\": \"Data mining\", \"text\": \" datum mining   be the process of discover pattern in large data set involve method at the intersection of machine learning , statistic , and database system . datum mining be an interdisciplinary subfield of computer science and statistic with an overall goal to extract information ( with intelligent method ) from a data set and transform the information into a comprehensible structure for further use . datum mining be the analysis step of the \\\" knowledge discovery in database \\\" process , or kdd . aside from the raw analysis step , it also involve database and data management aspect , datum pre - processing , model and inference consideration , interestingness metric , complexity consideration , post - processing of discover structure , visualization , and online updating . the difference between datum analysis and datum mining be that datum analysis be use to test model and hypothesis on the dataset , e.g. , analyze the effectiveness of a marketing campaign , regardless of the amount of datum ; in contrast , datum mining use machine - learning and statistical model to uncover clandestine or hide pattern in a large volume of datum . olson , d. l. ( 2007 ) . datum mining in business service . service business , 1(3 ) , 181 - 193 . doi:10.1007/s11628 - 006 - 0014 - 7 \", \"tokencount\": 227, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 6.852857873564791}, {\"docid\": 3, \"title\": \"Data mining\", \"text\": \" the term \\\" datum mining \\\" be in fact a misnomer , because the goal be the extraction of pattern and knowledge from large amount of datum , not the extraction ( mining ) of datum itself . It also be a buzzwordokairp 2005 fall conference , arizona state university   and be frequently apply to any form of large - scale datum or information processing ( collection , extraction , warehousing , analysis , and statistic ) as well as any application of computer decision support system , include artificial intelligence ( e.g. , machine learning ) and business intelligence . the book data mining : practical machine learning tool and technique with java ( which cover mostly machine learning material ) be originally to be name just practical machine learning , and the term datum mining be only add for marketing reason . often the more general term ( large scale ) data analysis and analytic \\u2013 or , when refer to actual method , artificial intelligence and machine learning \\u2013 be more appropriate . \", \"tokencount\": 177, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 3, \"title\": \"Data mining\", \"text\": \" the actual data mining task be the semi - automatic or automatic analysis of large quantity of datum to extract previously unknown , interesting pattern such as group of datum record ( cluster analysis ) , unusual record ( anomaly detection ) , and dependency ( association rule mining , sequential pattern mining ) . this usually involve use database technique such as spatial index . these pattern can then be see as a kind of summary of the input datum , and may be use in further analysis or , for example , in machine learning and predictive analytic . for example , the datum mining step may identify multiple group in the datum , which can then be use to obtain more accurate prediction result by a decision support system . neither the datum collection , datum preparation , nor result interpretation and reporting be part of the datum mining step , but do belong to the overall kdd process as additional step . \", \"tokencount\": 166, \"category\": \"Data mining\", \"lineid\": 2, \"term\": \"datum\", \"count\": 4.858351979352413}, {\"docid\": 3, \"title\": \"Data mining\", \"text\": \" the related term datum dredging , datum fishing , and datum snoop refer to the use of datum mining method to sample part of a large population datum set that be ( or may be ) too small for reliable statistical inference to be make about the validity of any pattern discover . these method can , however , be use in create new hypothesis to test against the large data population . \", \"tokencount\": 73, \"category\": \"Data mining\", \"lineid\": 3, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 10, \"title\": \"Pattern recognition\", \"text\": \" pattern recognition be the automate recognition of pattern and regularity in datum . pattern recognition be closely related to artificial intelligence and machine learning , \\n  together with application such as datum mining and knowledge discovery in database ( kdd ) , and be often use interchangeably with these term . however , these be distinguished : machine learning be one approach to pattern recognition , while other approach include hand - craft ( not learn ) rule or heuristic ; and pattern recognition be one approach to artificial intelligence , while other approach include symbolic artificial intelligence . a modern definition of pattern recognition be : \", \"tokencount\": 107, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 10, \"title\": \"Pattern recognition\", \"text\": \" this article focus on machine learn approach to pattern recognition . pattern recognition system be in many case train from label \\\" training \\\" datum ( supervised learning ) , but when no label datum be available other algorithm can be use to discover previously unknown pattern ( unsupervised learning ) . machine learning be the common term for supervised learning method and originate from artificial intelligence , whereas kdd and datum mining have a large focus on unsupervised method and strong connection to business use . pattern recognition have its origin in engineering , and the term be popular in the context of computer vision : a lead computer vision conference be name conference on computer vision and pattern recognition . in pattern recognition , there may be a high interest to formalize , explain and visualize the pattern , while machine learn traditionally focus on maximize the recognition rate . yet , all of these domain have evolve substantially from their root in artificial intelligence , engineering and statistic , and they have become increasingly similar by integrate development and idea from each other . \", \"tokencount\": 187, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 10, \"title\": \"Pattern recognition\", \"text\": \" in machine learning , pattern recognition be the assignment of a label to a give input value . in statistic , discriminant analysis be introduce for this same purpose in 1936 . an example of pattern recognition be classification , which attempt to assign each input value to one of a give set of class ( for example , determine whether a give email be \\\" spam \\\" or \\\" non - spam \\\" ) . however , pattern recognition be a more general problem that encompass other type of output as well . other example be regression , which assign a real - value output to each input ; sequence labeling , which assign a class to each member of a sequence of value ( for example , part of speech tagging , which assign a part of speech to each word in an input sentence ) ; and parse , which assign a parse tree to an input sentence , describe the syntactic structure of the sentence . \", \"tokencount\": 170, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 10, \"title\": \"Pattern recognition\", \"text\": \" pattern recognition algorithm generally aim to provide a reasonable answer for all possible input and to perform \\\" most likely \\\" matching of the input , take into account their statistical variation . this be oppose to pattern match algorithm , which look for exact match in the input with pre - exist pattern . a common example of a pattern - match algorithm be regular expression matching , which look for pattern of a give sort in textual datum and be include in the search capability of many text editor and word processor . in contrast to pattern recognition , pattern matching be not generally a type of machine learning , although pattern - match algorithm ( especially with fairly general , carefully tailor pattern ) can sometimes succeed in provide similar - quality output of the sort provide by pattern - recognition algorithm . \", \"tokencount\": 146, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 11, \"title\": \"SAS Institute\", \"text\": \" sas institute ( or sas , pronounce \\\" sass \\\" ) be an american multinational developer of analytic software base in cary , north carolina . sas develop and market a suite of analytic software ( also call sas ) , which help access , manage , analyze and report on datum to aid in decision - making . the company be the world 's large privately hold software business and its software be use by most of the fortune 500 . \", \"tokencount\": 82, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 11, \"title\": \"SAS Institute\", \"text\": \" sas institute start as a project at north carolina state university to create a statistical analysis system ( hence the proper name , statistical analysis system ) that be originally use primarily by agricultural department at university in the late 1960 . It become an independent , private business lead by current ceo james goodnight and three other project leader from the university in 1976 . sas grow from $ 10 million in revenue in 1980 to $ 1.1 billion by 2000 . a large proportion of these revenue be spend on research and development than at most other software company , at one point more than double the industry average . \", \"tokencount\": 112, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 13, \"title\": \"Early stopping\", \"text\": \" in machine learning , early stopping be a form of regularization use to avoid overfitt when train a learner with an iterative method , such as gradient descent . such method update the learner so as to make it better fit the training datum with each iteration . up to a point , this improve the learner 's performance on datum outside of the training set . past that point , however , improve the learner 's fit to the training datum come at the expense of increase generalization error . early stop rule provide guidance as to how many iteration can be run before the learner begin to over - fit . early stop rule have be employ in many different machine learning method , with vary amount of theoretical foundation . \", \"tokencount\": 133, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 14, \"title\": \"Machine learning\", \"text\": \" machine learning ( ml ) be the scientific study of algorithm and statistical model that computer system use in order to perform a specific task effectively without use explicit instruction , rely on pattern and inference instead . It be see as a subset of artificial intelligence . machine learn algorithm build a mathematical model base on sample datum , know as \\\" training datum \\\" , in order to make prediction or decision without be explicitly program to perform the task . machine learn algorithm be use in a wide variety of application , such as email filtering , and computer vision , where it be infeasible to develop an algorithm of specific instruction for perform the task . machine learning be closely related to computational statistic , which focus on make prediction use computer . the study of mathematical optimization deliver method , theory and application domain to the field of machine learning . datum mining be a field of study within machine learning , and focus on exploratory datum analysis through unsupervised learning . in its application across business problem , machine learning be also refer to as predictive analytic . \", \"tokencount\": 194, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 15, \"title\": \"Formal concept analysis\", \"text\": \" formal concept analysis ( fca ) be a principl way of derive a concept hierarchy or formal ontology from a collection of object and their property . each concept in the hierarchy represent the object share some set of property ; and each sub - concept in the hierarchy represent a subset of the object ( as well as a superset of the property ) in the concept above it . the term be introduce by rudolf wille in 1981 , and build on the mathematical theory of lattice and order set that be develop by garrett birkhoff and other in the 1930 . \", \"tokencount\": 104, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 15, \"title\": \"Formal concept analysis\", \"text\": \" formal concept analysis find practical application in field include datum mining , text mining , machine learning , knowledge management , semantic web , software development , chemistry and biology . \", \"tokencount\": 31, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" in time series analysis , dynamic time warping ( dtw ) be one of the algorithm for measure similarity between two temporal sequence , which may vary in speed .   for instance , similarity in walking could be detect use dtw , even if one person be walk faster than the other , or if there be acceleration and deceleration during the course of an observation . dtw have be apply to temporal sequence of video , audio , and graphic datum -- indeed , any datum that can be turn into a linear sequence can be analyze with dtw . a well know application have be automatic speech recognition , to cope with different speaking speed . other application include speaker recognition and online signature recognition . also it be see that it can be use in partial shape matching application . \", \"tokencount\": 144, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" in general , dtw be a method that calculate an optimal match between two give sequence ( e.g. time series ) with certain restriction and rule : \\n  every index from the first sequence must be match with one or more index from the other sequence , and vice versa \\n  the first index from the first sequence must be match with the first index from the other sequence ( but it do not have to be its only match ) \\n  the last index from the first sequence must be match with the last index from the other sequence ( but it do not have to be its only match ) \\n  the mapping of the index from the first sequence to index from the other sequence must be monotonically increase , and vice versa , i.e. if j > i be index from the first sequence , then there must not be two index l > k in the other sequence , such that index i be match with index l and index j be match with index k , and vice versa \", \"tokencount\": 185, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" the optimal match be denote by the match that satisfy all the restriction and the rule and that have the minimal cost , where the cost be compute as the sum of absolute difference , for each match pair of index , between their value . \", \"tokencount\": 46, \"category\": \"Machine learning algorithms\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" the sequence be \\\" warped \\\" non - linearly in the time dimension to determine a measure of their similarity independent of certain non - linear variation in the time dimension . this sequence alignment method be often use in time series classification . although dtw measure a distance - like quantity between two give sequence , it do not guarantee the triangle inequality to hold . \", \"tokencount\": 67, \"category\": \"Machine learning algorithms\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" in addition to a similarity measure between the two sequence , a so call \\\" warp path \\\" be produce , by warp accord to this path the two signal may be align in time . the signal with an original set of point x(original ) , y(original ) be transform to x(warped ) , y(warped ) . this find application in genetic sequence and audio synchronisation . in a related technique sequence of vary speed may be average use this technique see the average sequence section . \", \"tokencount\": 88, \"category\": \"Machine learning algorithms\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 16, \"title\": \"Dynamic time warping\", \"text\": \" this be conceptually very similar to the needleman \\u2013 wunsch algorithm , which be explain in great detail . \", \"tokencount\": 19, \"category\": \"Machine learning algorithms\", \"lineid\": 5, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 17, \"title\": \"R (programming language)\", \"text\": \" r be a programming language and free software environment for statistical computing and graphic support by the r foundation for statistical computing . the r language be widely use among statistician and datum miner for develop statistical software and datum analysis . poll , datum mining survey , and study of scholarly literature database show substantial increase in popularity ;   r rank 22nd in the tiobe index , a measure of popularity of programming language . \", \"tokencount\": 77, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 17, \"title\": \"R (programming language)\", \"text\": \" a gnu package , source code for the r software environment be write primarily in c , fortran and r itself , and be freely available under the gnu general public license . pre - compile binary version be provide for various operating system . although r have a command line interface , there be several graphical user interface , such as rstudio , an integrate development environment . \", \"tokencount\": 69, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 19, \"title\": \"Visual inspection\", \"text\": \" visual inspection be a common method of quality control , datum acquisition , and datum analysis . visual inspection , use in maintenance of facility , mean inspection of equipment and structure use either or all of raw human sens such as vision , hearing , touch and smell and/or any non - specialize inspection equipment . \\n inspections require ultrasonic , x - ray equipment , infra - red , etc . be not typically regard as visual inspection as these inspection methodology require specialized equipment , training and certification . \", \"tokencount\": 92, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 28, \"title\": \"Cluster analysis\", \"text\": \" cluster analysis or clustering be the task of group a set of object in such a way that object in the same group ( call a cluster ) be more similar ( in some sense ) to each other than to those in other group ( cluster ) . It be a main task of exploratory datum mining , and a common technique for statistical datum analysis , use in many field , include machine learning , pattern recognition , image analysis , information retrieval , bioinformatic , datum compression , and computer graphic . \", \"tokencount\": 95, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 28, \"title\": \"Cluster analysis\", \"text\": \" cluster analysis itself be not one specific algorithm , but the general task to be solve . It can be achieve by various algorithm that differ significantly in their understanding of what constitute a cluster and how to efficiently find them . popular notion of cluster include group with small distance between cluster member , dense area of the data space , interval or particular statistical distribution . clustering can therefore be formulate as a multi - objective optimization problem . the appropriate clustering algorithm and parameter setting ( include parameter such as the distance function to use , a density threshold or the number of expect cluster ) depend on the individual datum set and intend use of the result . cluster analysis as such be not an automatic task , but an iterative process of knowledge discovery or interactive multi - objective optimization that involve trial and failure . It be often necessary to modify datum preprocess and model parameter until the result achieve the desire property . \", \"tokencount\": 170, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 28, \"title\": \"Cluster analysis\", \"text\": \" besides the term cluster , there be a number of term with similar meaning , include automatic classification , numerical taxonomy , botryology ( from greek \\u03b2\\u03cc\\u03c4\\u03c1\\u03c5\\u03c2 \\\" grape \\\" ) , typological analysis , and community detection . the subtle difference be often in the use of the result : while in datum mining , the result group be the matter of interest , in automatic classification the result discriminative power be of interest . \", \"tokencount\": 76, \"category\": \"Data mining\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 28, \"title\": \"Cluster analysis\", \"text\": \" cluster analysis be originate in anthropology by driver and kroeber in 1932 and introduce to psychology by joseph zubin in 1938 and robert tryon in 1939 and famously use by cattell begin in 1943 for trait theory classification in personality psychology . \", \"tokencount\": 42, \"category\": \"Data mining\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 29, \"title\": \"Curse of dimensionality\", \"text\": \" the curse of dimensionality refer to various phenomenon that arise when analyze and organize datum in high - dimensional space ( often with hundred or thousand of dimension ) that do not occur in low - dimensional setting such as the three - dimensional physical space of everyday experience . the expression be coin by richard e. bellman when consider problem in dynamic programming.,republish : \", \"tokencount\": 65, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 29, \"title\": \"Curse of dimensionality\", \"text\": \" cursed phenomenon occur in domain such as numerical analysis , sampling , combinatoric , machine learning , datum mining and database . the common theme of these problem be that when the dimensionality increase , the volume of the space increase so fast that the available datum become sparse . this sparsity be problematic for any method that require statistical significance . in order to obtain a statistically sound and reliable result , the amount of datum need to support the result often grow exponentially with the dimensionality . also , organizing and search datum often rely on detect area where object form group with similar property ; in high dimensional datum , however , all object appear to be sparse and dissimilar in many way , which prevent common datum organization strategy from be efficient . \", \"tokencount\": 137, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 30, \"title\": \"Data profiling\", \"text\": \" datum profiling be the process of examine the datum available from an exist information source ( e.g. a database or a file ) and collect statistic or informative summary about that datum . the purpose of these statistic may be to : \\n  find out whether exist datum can be easily use for other purpose \\n  improve the ability to search datum by tag it with keyword , description , or assign it to a category \\n  assess datum quality , include whether the datum conform to particular standard or pattern \\n  assess the risk involve in integrate datum in new application , include the challenge of join \\n  discover metadata of the source database , include value pattern and distribution , key candidate , foreign - key candidate , and functional dependency \\n  assess whether know metadata accurately describe the actual value in the source database \\n  understanding data challenge early in any datum intensive project , so that late project surprise be avoid . find data problem late in the project can lead to delay and cost overrun . \\n  have an enterprise view of all datum , for us such as master data management , where key data be need , or data governance for improve datum quality . \", \"tokencount\": 212, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 6.852857873564791}, {\"docid\": 31, \"title\": \"Web mining\", \"text\": \" web mining be the application of datum mining technique to discover pattern from the world wide web . as the name propose , this be information gather by mine the web . It make utilization of automate apparatus to reveal and extricate datum from server and web2 report , and it permit organization to get to both organized and unstructured information from browser activity , server log , website and link structure , page content and different source . \", \"tokencount\": 79, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 31, \"title\": \"Web mining\", \"text\": \" the goal of web structure mining be to generate structural summary about the web site and web page . technically , web content mining mainly focus on the structure of inner - document , while web structure mining try to discover the link structure of the hyperlink at the inter - document level . base on the topology of the hyperlink , web structure mining will categorize the web page and generate the information , such as the similarity and relationship between different web site . \", \"tokencount\": 86, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 31, \"title\": \"Web mining\", \"text\": \" web structure mining can also have another direction -- discover the structure of web document itself . this type of structure mining can be use to reveal the structure ( schema ) of web page , this would be good for navigation purpose and make it possible to compare / integrate web page scheme . this type of structure mining will facilitate introduce database technique for access information in web page by provide a reference schema . \", \"tokencount\": 77, \"category\": \"Data mining\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 33, \"title\": \"Receiver operating characteristic\", \"text\": \" a receiver operate characteristic curve , or roc curve , be a graphical plot that illustrate the diagnostic ability of a binary classifier system as its discrimination threshold be varied . \", \"tokencount\": 31, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 33, \"title\": \"Receiver operating characteristic\", \"text\": \" the roc curve be create by plot the true positive rate ( tpr ) against the false positive rate ( fpr ) at various threshold setting . the true - positive rate be also know as sensitivity , recall or probability of detection in machine learning . the false - positive rate be also know as the fall - out or probability of false alarm and can be calculate as ( 1 \\u2212 specificity ) . It can also be think of as a plot of the power as a function of the type i error of the decision rule ( when the performance be calculate from just a sample of the population , it can be think of as estimator of these quantity ) . the roc curve be thus the sensitivity as a function of fall - out . in general , if the probability distribution for both detection and false alarm be know , the roc curve can be generate by plot the cumulative distribution function ( area under the probability distribution from -\\\\infty to the discrimination threshold ) of the detection probability in the y - axis versus the cumulative distribution function of the false - alarm probability on the x - axis . \", \"tokencount\": 208, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 33, \"title\": \"Receiver operating characteristic\", \"text\": \" roc analysis provide tool to select possibly optimal model and to discard suboptimal one independently from ( and prior to specify ) the cost context or the class distribution . roc analysis be relate in a direct and natural way to cost / benefit analysis of diagnostic decision making . \", \"tokencount\": 50, \"category\": \"Data mining\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 33, \"title\": \"Receiver operating characteristic\", \"text\": \" the roc curve be first develop by electrical engineer and radar engineer during world war ii for detect enemy object in battlefield and be soon introduce to psychology to account for perceptual detection of stimulus . roc analysis since then have be use in medicine , radiology , biometric , forecasting of natural hazard , meteorology , model performance assessment , and other area for many decade and be increasingly use in machine learning and datum mining research . \", \"tokencount\": 79, \"category\": \"Data mining\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 33, \"title\": \"Receiver operating characteristic\", \"text\": \" the roc be also know as a relative operating characteristic curve , because it be a comparison of two operating characteristic ( tpr and fpr ) as the criterion change . swet , john a. ; signal detection theory and roc analysis in psychology and diagnostic : collect paper , lawrence erlbaum associates , mahwah , nj , 1996 \", \"tokencount\": 59, \"category\": \"Data mining\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 34, \"title\": \"Relational data mining\", \"text\": \" relational datum mining be the data mining technique for relationaldatabas . dzeroski , saso , lavra\\u010d , nada ( eds . ) , relational data mining , springer 2001   unlike traditional datum mining algorithm , which look for \\n pattern in a single table ( propositional pattern ) , \\n relational datum mining algorithm look for pattern among multiple table \\n ( relational pattern ) . for most type of propositional \\n pattern , there be correspond relational pattern . for example , \\n there be relational classification rule ( relational classification ) , relational regression tree , and relational association rule . \", \"tokencount\": 104, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 34, \"title\": \"Relational data mining\", \"text\": \" there be several approach to relational datum mining : \\n  inductive logic programming ( ilp ) \\n  statistical relational learning ( srl ) \\n  graph mining \\n  propositionalization \\n  multi - view learning \", \"tokencount\": 33, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 36, \"title\": \"Savi Technology\", \"text\": \" savi technology be found in 1989 and be base in alexandria , virginia . \", \"tokencount\": 14, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 36, \"title\": \"Savi Technology\", \"text\": \" savi deliver live streaming fact and insight about the location , condition , and security of in - transit good . use big datum analytic , savi equip shipper , carrier , 3pls , and government to optimize supply chain logistic before , during and after transit , reduce cost and inventory while improve service . savi be trust to run the world \\u2019s large and most complex asset tracking and monitoring network serve the us dod , allied military and more than 1000 commercial company around the globe . \", \"tokencount\": 90, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 37, \"title\": \"ROOT\", \"text\": \" root be an object - orient program and library develop by cern . It be originally design for particle physics datum analysis and contain several feature specific to this field , but it be also use in other application such as astronomy and datum mining .   the late release be 6.18.00 , as of 2019 - 06 - 25 . \", \"tokencount\": 61, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 38, \"title\": \"Statistical learning theory\", \"text\": \" statistical learning theory be a framework for machine learn \\n draw from the field of statistic and functional analysis . trevor hastie , robert tibshirani , jerome friedman ( 2009 ) the elements of statistical learning , springer - verlag . statistical learn theory deal with the problem of find a predictive function base on datum . statistical learning theory have lead to successful application in field such as computer vision , speech recognition , bioinformatic and baseball . gagan sidhu , brian caffo . exploit pitcher decision - making use reinforcement learning . annals of applied statistics \", \"tokencount\": 98, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 46, \"title\": \"Data dredging\", \"text\": \" datum dredging ( also datum fishing , datum snooping , datum butchery , and p - hacking ) be the misuse of datum analysis to find pattern in datum that can be present as statistically significant when in fact there be no real underlying effect . this be do by perform many statistical test on the datum and only pay attention to those that come back with significant result , instead of state a single hypothesis about an underlying effect before the analysis and then conduct a single test for it . \", \"tokencount\": 92, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 4.19351668128162}, {\"docid\": 46, \"title\": \"Data dredging\", \"text\": \" the process of datum dredging involve automatically test huge number of hypothesis about a single datum set by exhaustively search \\u2014 perhaps for combination of variable that may show a correlation , and perhaps for group of case or observation that show difference in their mean or in their breakdown by some other variable . \", \"tokencount\": 55, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 46, \"title\": \"Data dredging\", \"text\": \" conventional test of statistical significance be base on the probability that a particular result would arise if chance alone be at work , and necessarily accept some risk of mistaken conclusion of a certain type ( mistaken rejection of the null hypothesis ) .   this level of risk be call the significance . when large number of test be perform , some produce false result of this type , hence 5 % of randomly choose hypothesis turn out to be significant at the 5 % level , 1 % turn out to be significant at the 1 % significance level , and so on , by chance alone . when enough hypothesis be test , it be virtually certain that some will be statistically significant but misleading , since almost every datum set with any degree of randomness be likely to contain ( for example ) some spurious correlation . if they be not cautious , researcher use datum mining technique can be easily mislead by these result . \", \"tokencount\": 170, \"category\": \"Data mining\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 46, \"title\": \"Data dredging\", \"text\": \" the multiple comparison hazard be common in datum dredging . moreover , subgroup be sometimes explore without alert the reader to the number of question at issue , which can lead to misinform conclusion . \\n \", \"tokencount\": 36, \"category\": \"Data mining\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" opendap be an acronym for \\\" open - source project for a network data access protocol , \\\" an endeavor focus on enhance the retrieval of remote , structured datum through a web - base architecture and a discipline - neutral data access protocol ( dap ) . widely use , especially in earth science , the protocol be layer on http , and its current specification be dap4,dap4 specification though the previous dap2 version remain broadly use . develop and advanced ( openly and collaboratively ) by the non - profit   opendap , inc.,official website dap be intend to enable remote , selective datum - retrieval as an easily invoke web service . opendap , inc. also develop and maintain zero - cost ( reference ) implementation of the dap protocol in both server - side and client - side software . \\n \\\" opendap \\\" often be use in place of \\\" dap \\\" to denote the protocol but also may refer to an entire dap - base data - retrieval architecture . other dap - center architecture , such as threddsthredds and erddap , the noaa geo - ide uaf erddaperddap exhibit significant interoperability with one another as well as with system employ opendap 's own ( open - source ) server and software . \", \"tokencount\": 219, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" a dap client can be an ordinary browser or even a spreadsheet , though with limited functionality ( see opendap 's web page on available client software ) . more typically , dap client be : \\n  data - analysis or data - visualization tool ( such as matlab , idl , panoply , grads , integrated data viewer , ferret and ncbrowsea graphical netcdf file browser ) which their author have adapt to enable dap - base data input ; \\n  similarly adapt web application ( such as dapper data viewer , aka dchart)opendap software \\n  similarly adapt end - user program ( in common language ) \", \"tokencount\": 108, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" regardless of their type , and whether develop commercially or by an end - user , client almost universally link to dap server through library that implement the dap2 or dap4 protocol in one language or another . opendap offer open - source library in c++ and java , but many client rely on community develop library such as pydap or , especially , the netcdf suite . develop and maintain by the unidata program at the ucar in multiple programming language , all netcdf library include embed capability for retrieve ( array - style ) datum from dap server . \", \"tokencount\": 101, \"category\": \"Data analysis\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" a data - use client reference a data set by its url and request metadata or content by issue ( usually through an embed dap library ) an http request to a dap server . content request usually be precede by request for metadata describe the structure and other detail about the reference data set . with this information , the client may construct dap constraint expressionsdap constraint expression to retrieve specific content ( i.e. , subset ) from the source . opendap server offer various type of response , depend on the specific form of the client 's request , include xml , json , html and ascii . in response to request for content , opendap server can respond with multi - part mime document that include a binary portion with netcdf or dap - native encoding . ( these binary form offer compact mean to deliver large volume of content , and the dap - native form may even be stream if desire . ) \", \"tokencount\": 168, \"category\": \"Data analysis\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" opendap 's software for build dap server ( on top of apache ) be dub hyrax and include adapter that facilitate serve a wide variety of source datum . dap server most frequently enable ( remote ) access to ( large ) hdf or netcdf file , but the source datum can exist in database or other format , include user - define one . when source datum be organize as file , dap retrieval enable , via subsetting , finer - grain access than do the ftp . furthermore , opendap server can aggregate subset from multiple file for delivery in a single retrieval . take together , subsetting , aggregation and streaming can yield substantial datum - access efficiency , even in the presence of slow network . \", \"tokencount\": 130, \"category\": \"Data analysis\", \"lineid\": 4, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 49, \"title\": \"OPeNDAP\", \"text\": \" opendap and other dap server be use operationally in government agency , include nasa and noaa , for provide access to earth science datum , include satellite imagery and other high - volume information source . the dap data model embrace a comprehensive set of datum structure , include multidimensional array and nest sequence ( i.e. , record ) , complement by a correspondingly rich set of constraint expression . hence the opendap data - retrieval architecture have demonstrate utility across a broad range of scientific datum type , include datum generate via simulation and datum generate via observation ( whether remotely sense or measure in situ ) . \", \"tokencount\": 109, \"category\": \"Data analysis\", \"lineid\": 5, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 52, \"title\": \"Training, validation, and test sets\", \"text\": \" in machine learning , a common task be the study and construction of algorithm that can learn from and make prediction on datum . such algorithms work by make data - drive prediction or decision , machine learning and pattern recognition \\\" can be view as two facet of the same field . \\\" through build a mathematical model from input datum . \", \"tokencount\": 63, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 52, \"title\": \"Training, validation, and test sets\", \"text\": \" the datum use to build the final model usually come from multiple dataset . in particular , three datum set be commonly use in different stage of the creation of the model . \", \"tokencount\": 33, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 52, \"title\": \"Training, validation, and test sets\", \"text\": \" the model be initially fit on a training dataset , that be a set of example use to fit the parameter ( e.g. weight of connection between neuron in artificial neural network ) of the model . the model ( e.g. a neural net or a naive baye classifier ) be train on the training dataset use a supervised learning method ( e.g. gradient descent or stochastic gradient descent ) . in practice , the training dataset often consist of pair of an input vector ( or scalar ) and the correspond output vector ( or scalar ) , which be commonly denote as the target ( or label ) . the current model be run with the training dataset and produce a result , which be then compare with the target , for each input vector in the training dataset . base on the result of the comparison and the specific learning algorithm be use , the parameter of the model be adjust . the model fitting can include both variable selection and parameter estimation . \", \"tokencount\": 177, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 52, \"title\": \"Training, validation, and test sets\", \"text\": \" successively , the fit model be use to predict the response for the observation in a second dataset call the validation dataset . the validation dataset provide an unbiased evaluation of a model fit on the training dataset while tune the model 's hyperparameter   ( e.g. the number of hide unit in a neural network ) . validation dataset can be use for regularization by early stopping : stop training when the error on the validation dataset increase , as this be a sign of overfitt to the training dataset . \\n this simple procedure be complicate in practice by the fact that the validation dataset 's error may fluctuate during training , produce multiple local minima . this complication have lead to the creation of many ad - hoc rule for decide when overfitt have truly begin . \", \"tokencount\": 140, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 52, \"title\": \"Training, validation, and test sets\", \"text\": \" finally , the test dataset be a dataset use to provide an unbiased evaluation of a final model fit on the training dataset . if the datum in the test dataset have never be use in training ( for example in cross - validation ) , the test dataset be also call a holdout dataset . \", \"tokencount\": 56, \"category\": \"Machine learning\", \"lineid\": 4, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 54, \"title\": \"Statistical classification\", \"text\": \" in machine learning and statistic , classification be the problem of identify to which of a set of category ( sub - population ) a new observation belong , on the basis of a training set of datum contain observation ( or instance ) whose category membership be know .   examples be assign a give email to the \\\" spam \\\" or \\\" non - spam \\\" class , and assign a diagnosis to a give patient base on observed characteristic of the patient ( sex , blood pressure , presence or absence of certain symptom , etc . ) .   classification be an example of pattern recognition . \", \"tokencount\": 111, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 54, \"title\": \"Statistical classification\", \"text\": \" in the terminology of machine learning , classification be consider an instance of supervised learning , i.e. , learn where a training set of correctly identify observation be available .   the correspond unsupervised procedure be know as cluster , and involve group datum into category base on some measure of inherent similarity or distance . \", \"tokencount\": 56, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 54, \"title\": \"Statistical classification\", \"text\": \" often , the individual observation be analyze into a set of quantifiable property , know variously as explanatory variable or feature .   these property may variously be categorical ( e.g. \\\" a \\\" , \\\" b \\\" , \\\" ab \\\" or \\\" o \\\" , for blood type ) , ordinal ( e.g. \\\" large \\\" , \\\" medium \\\" or \\\" small \\\" ) , integer - value ( e.g. the number of occurrence of a particular word in an email ) or real - value ( e.g. a measurement of blood pressure ) . other classifier work by compare observation to previous observation by mean of a similarity or distance function . \", \"tokencount\": 116, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 54, \"title\": \"Statistical classification\", \"text\": \" an algorithm that implement classification , especially in a concrete implementation , be know as a classifier .   the term \\\" classifi \\\" sometimes also refer to the mathematical function , implement by a classification algorithm , that map input datum to a category . \", \"tokencount\": 46, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 54, \"title\": \"Statistical classification\", \"text\": \" terminology across field be quite varied . in statistic , where classification be often do with logistic regression or a similar procedure , the property of observation be term explanatory variable ( or independent variable , regressor , etc . ) , and the category to be predict be know as outcome , which be consider to be possible value of the dependent variable .   in machine learning , the observation be often know as instance , the explanatory variable be term feature ( group into a feature vector ) , and the possible category to be predict be class .   other field may use different terminology : e.g. in community ecology , the term \\\" classification \\\" normally refer to cluster analysis , i.e. , a type of unsupervised learning , rather than the supervised learning describe in this article . \", \"tokencount\": 144, \"category\": \"Machine learning\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 55, \"title\": \"Natural Language Toolkit\", \"text\": \" the natural language toolkit , or more commonly nltk , be a suite of library and program for symbolic and statistical natural language processing ( nlp ) for english write in the python programming language . It be develop by steven bird and edward loper in the department of computer and information science at the university of pennsylvania . nltk include graphical demonstration and sample datum . It be accompany by a book that explain the underlie concept behind the language processing task support by the toolkit , plus a cookbook . \", \"tokencount\": 92, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 55, \"title\": \"Natural Language Toolkit\", \"text\": \" nltk be intend to support research and teaching in nlp or closely related area , include empirical linguistic , cognitive science , artificial intelligence , information retrieval , and machine learning . \\n nltk have be use successfully as a teaching tool , as an individual study tool , and as a platform for prototyp and building research system . there be 32 university in the us and 25 country use nltk in their course . nltk support classification , tokenization , stem , tag , parse , and semantic reasoning functionality . \", \"tokencount\": 93, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 57, \"title\": \"Data stream mining\", \"text\": \" data stream mining be the process of extract knowledge structure from continuous , rapid data record . a data stream be an order sequence of instance that in many application of data stream mining can be read only once or a small number of time use limited computing and storage capability . \\n in many data stream mining application , the goal be to predict the class or value of new instance in the data stream give some knowledge about the class membership or value of previous instance in the data stream . \\n machine learning technique can be use to learn this prediction task from label example in an automate fashion . \\n often , concept from the field of incremental learning be apply to cope with structural change , on - line learn and real - time demand . \\n in many application , especially operate within non - stationary environment , the distribution underlie the instance or the rule underlie their labeling may change over time , i.e. the goal of the prediction , the class to be predict or the target value to be predict , may change over time . this problem be refer to as concept drift . \", \"tokencount\": 204, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 57, \"title\": \"Data stream mining\", \"text\": \" example of datum stream include computer network traffic , phone conversation , atm transaction , web search , and sensor datum . \\n data stream mining can be consider a subfield of datum mining , machine learning , and knowledge discovery . \", \"tokencount\": 42, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \" in pattern recognition , the k - near neighbor algorithm ( k - nn ) be a non - parametric method use for classification and regression . in both case , the input consist of the k close training example in the feature space . the output depend on whether k - nn be use for classification or regression : \", \"tokencount\": 60, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \"   in k - nn classification , the output be a class membership . an object be classify by a plurality vote of its neighbor , with the object be assign to the class most common among its k near neighbor ( k be a positive integer , typically small ) . if k    =    1 , then the object be simply assign to the class of that single near neighbor . \", \"tokencount\": 74, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \"   in k - nn regression , the output be the property value for the object . this value be the average of the value of k near neighbor . \", \"tokencount\": 30, \"category\": \"Machine learning algorithms\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \" k - nn be a type of instance - base learning , or lazy learning , where the function be only approximate locally and all computation be defer until classification . the k - nn algorithm be among the simple of all machine learn algorithm . \", \"tokencount\": 46, \"category\": \"Machine learning algorithms\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \" both for classification and regression , a useful technique can be use to assign weight to the contribution of the neighbor , so that the near neighbor contribute more to the average than the more distant one . for example , a common weighting scheme consist in give each neighbor a weight of 1/d , where d be the distance to the neighbor . this scheme be a generalization of linear interpolation . \", \"tokencount\": 73, \"category\": \"Machine learning algorithms\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \" the neighbor be take from a set of object for which the class ( for k - nn classification ) or the object property value ( for k - nn regression ) be know . this can be think of as the training set for the algorithm , though no explicit training step be require . \", \"tokencount\": 56, \"category\": \"Machine learning algorithms\", \"lineid\": 5, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 58, \"title\": \"K-nearest neighbors algorithm\", \"text\": \" a peculiarity of the k - nn algorithm be that it be sensitive to the local structure of the datum . \", \"tokencount\": 21, \"category\": \"Machine learning algorithms\", \"lineid\": 6, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 59, \"title\": \"Linde\\u2013Buzo\\u2013Gray algorithm\", \"text\": \" the linde \\u2013 buzo \\u2013 gray algorithm ( introduce by yoseph linde , andr\\u00e9s buzo and robert m. gray in 1980 ) be a vector quantization algorithm to derive a good codebook . \", \"tokencount\": 33, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 59, \"title\": \"Linde\\u2013Buzo\\u2013Gray algorithm\", \"text\": \" It be similar to the k - means method in datum cluster . \", \"tokencount\": 13, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 60, \"title\": \"JMP (statistical software)\", \"text\": \" jmp ( pronounce \\\" jump \\\" ) be a suite of computer program for statistical analysis develop by the jmp business unit of sas institute . It be launch in 1989 to take advantage of the graphical user interface introduce by the macintosh . It have since be significantly rewrite and make available for the windows operate system . jmp be use in application such as six sigma , quality control , and engineering , design of experiment , as well as for research in science , engineering , and social science . \", \"tokencount\": 93, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 60, \"title\": \"JMP (statistical software)\", \"text\": \" the software can be purchase in any of five configuration : jmp , jmp pro , jmp clinical , jmp genomics and the jmp graph builder app for the ipad . jmp can be automate with its proprietary scripting language , jsl . the software be focus on exploratory visual analytic , where user investigate and explore datum . these exploration can also be verify by hypothesis testing , datum mining , or other analytic method . in addition , discovery make through graphical exploration can lead to a design experiment that can be both design and analyze with jmp . \", \"tokencount\": 101, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 61, \"title\": \"Kernel density estimation\", \"text\": \" in statistic , kernel density estimation ( kde ) be a non - parametric way to estimate the probability density function of a random variable .   kernel density estimation be a fundamental datum smoothing problem where inference about the population be make , base on a finite data sample . in some field such as signal processing and econometric it be also term the parzen \\u2013 rosenblatt window method ,   after emanuel parzen and murray rosenblatt , who be usually credit with independently create it in its current form . \", \"tokencount\": 92, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 64, \"title\": \"Pivot table\", \"text\": \" a pivot table be a table of statistic that summarize   the datum of a more extensive table ( such as from a database , spreadsheet , or business intelligence program ) . this summary may include sum , average , or other statistic , which the pivot table group together in a meaningful way . \", \"tokencount\": 56, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 64, \"title\": \"Pivot table\", \"text\": \" pivot table be a technique in datum processing . They enable a person to arrange and rearrange ( or \\\" pivot \\\" ) statistic in order to draw attention to useful information . \", \"tokencount\": 33, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 64, \"title\": \"Pivot table\", \"text\": \" although pivot table be a generic term , microsoft corporation trademark pivottable in the united states in 1994 . \", \"tokencount\": 19, \"category\": \"Data analysis\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 65, \"title\": \"Orange (software)\", \"text\": \" orange be an open - source datum visualization , machine learning and datum mining toolkit . It feature a visual programming front - end for explorative datum analysis and interactive datum visualization , and can also be use as a python library . \", \"tokencount\": 43, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 66, \"title\": \"Microsoft Analysis Services\", \"text\": \" microsoft sql server analysis services , ssas , tableau frequently refer to ssas cubes as msas cubes be an online analytical processing ( olap ) and datum mining tool in microsoft sql server . ssas be use as a tool by organization to analyze and make sense of information possibly spread out across multiple database , or in disparate table or file . microsoft have include a number of service in sql server relate to business intelligence and datum warehousing . these service include integration services , reporting services and analysis services . analysis services include a group of olap and datum mining capability and come in two flavor - multidimensional and tabular . \", \"tokencount\": 114, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 67, \"title\": \"Data analysis\", \"text\": \" datum analysis be a process of inspect , cleanse , transform , and modeling datum with the goal of discover useful information , inform conclusion , and support decision - making . datum analysis have multiple facet and approach , encompass diverse technique under a variety of name , and be use in different business , science , and social science domain . in today 's business world , datum analysis play a role in make decision more scientific and help business operate more effectively . xia , b. s. , & gong , p. ( 2015 ) . review of business intelligence through datum analysis . benchmarking , 21(2 ) , 300 - 311 . doi:10.1108/bij-08 - 2012 - 0050 \", \"tokencount\": 121, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 67, \"title\": \"Data analysis\", \"text\": \" datum mining be a particular data analysis technique that focus on modeling and knowledge discovery for predictive rather than purely descriptive purpose , while business intelligence cover datum analysis that rely heavily on aggregation , focus mainly on business information . exploring data analysis in statistical application , datum analysis can be divide into descriptive statistic , exploratory datum analysis ( eda ) , and confirmatory datum analysis ( cda ) . eda focus on discover new feature in the datum while cda focus on confirm or falsify exist hypothesis . predictive analytic focus on application of statistical model for predictive forecasting or classification , while text analytic apply statistical , linguistic , and structural technique to extract and classify information from textual source , a species of unstructured datum . all of the above be variety of datum analysis . \", \"tokencount\": 141, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 4.858351979352413}, {\"docid\": 67, \"title\": \"Data analysis\", \"text\": \" data integration be a precursor to datum analysis , and datum analysis be closely link to datum visualization and datum dissemination . the term datum analysis be sometimes use as a synonym for datum modeling . \", \"tokencount\": 36, \"category\": \"Data analysis\", \"lineid\": 2, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 72, \"title\": \"Multifactor dimensionality reduction\", \"text\": \" multifactor dimensionality reduction ( mdr ) be a statistical approach , also use in machine learn automatic approach , for detect and characterize combination of attribute or independent variable that interact to influence a dependent or class variable . mdr be design specifically to identify nonadditive interaction among discrete variable that influence a binary outcome and be consider a nonparametric and model - free alternative to traditional statistical method such as logistic regression . \\n the basis of the mdr method be a constructive induction or feature engineering algorithm that convert two or more variable or attribute to a single attribute .   this process of construct a new attribute change the representation space of the datum .   the end goal be to create or discover a representation that facilitate the detection of nonlinear or nonadditive interaction among the attribute such that prediction of the class variable be improve over that of the original representation of the datum . \", \"tokencount\": 160, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 74, \"title\": \"Robot learning\", \"text\": \" robot learn be a research field at the intersection of machine learning and robotic . It study technique allow a robot to acquire novel skill or adapt to its environment through learn algorithm . the embodiment of the robot , situate in a physical embedding , provide at the same time specific difficulty ( e.g. high - dimensionality , real time constraint for collect datum and learning ) and opportunity for guide the learning process ( e.g. sensorimotor synergy , motor primitive ) . \\n example of skill that be target by learn algorithm include sensorimotor skill such as locomotion , grasp , active object categorization ,   as well as interactive skill such as joint manipulation of an object with a human peer , and linguistic skill such as the ground and situate meaning of human language . learning can happen either through autonomous self - exploration or through guidance from a human teacher , like for example in robot learn by imitation . \", \"tokencount\": 165, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 74, \"title\": \"Robot learning\", \"text\": \" robot learning can be closely relate to adaptive control , reinforcement learning as well as developmental robotic which consider the problem of autonomous lifelong acquisition of repertoire of skill . \\n while machine learning be frequently use by computer vision algorithm employ in the context of robotic , these application be usually not refer to as \\\" robot learning \\\" . \", \"tokencount\": 61, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 75, \"title\": \"Vasant Honavar\", \"text\": \" vasant g. honavar be an indian bear american computer scientist , and artificial intelligence , machine learning , big datum , data science , causality , knowledge representation , bioinformatic and health informatic researcher and educator . \", \"tokencount\": 37, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 78, \"title\": \"Fityk\", \"text\": \" fityk be a curve fitting and datum analysis application , predominantly use to fit analytical , \\n bell - shape function to experimental datum . It be position to fill the gap between general plot software and program specific for one field , e.g. crystallography or xps . \", \"tokencount\": 48, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 78, \"title\": \"Fityk\", \"text\": \" originally , fityk be develop to analyse powder diffraction datum . It be also use in other field that require peak analysis and peak - fitting , like chromatography or various kind of spectroscopy . \", \"tokencount\": 35, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 78, \"title\": \"Fityk\", \"text\": \" fityk be distribute under the term of gnu general public license , but since version 1.0.0 , subscription be require for download binary . It run on linux , macos , microsoft windows , freebsd and other platform . It operate either as a command line program or with a graphical user interface . \", \"tokencount\": 54, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 78, \"title\": \"Fityk\", \"text\": \" It be write in c++ , use wxwidget , and provide binding for python and other scripting language . \", \"tokencount\": 19, \"category\": \"Data analysis software\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 79, \"title\": \"Ross Quinlan\", \"text\": \" john ross quinlan be a computer science researcher in datum mining and decision theory . He have contribute extensively to the development of decision tree algorithm , include invent the canonical c4.5 and id3 algorithm . He also contribute to early ilp literature with first order inductive learner ( foil ) . He be currently run the company rulequest research which he found in 1997 . \", \"tokencount\": 66, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 83, \"title\": \"Random mapping\", \"text\": \" when the datum vector be high - dimensional it be computationally infeasible to use datum analysis or pattern recognition algorithm which repeatedly compute similarity or distance in the original data space . It be therefore necessary to reduce the dimensionality before , for example , cluster the datum . \\n random mapping ( rm ) be a fast dimensionality reduction method   categorize as feature extraction method . the rm consist in generation of a random matrix that be multiply by each original vector and result in a reduced vector . \\n in text mining context , it be demonstrate that the document classification accuracy obtain after the dimensionality have be reduce use a random mapping method will be almost as good as the original accuracy if the final dimensionality be sufficiently large ( about 100 out of 6000 ) . in fact , it can be show that the inner product ( similarity ) between the map vector follow closely the inner product of the original vector . \", \"tokencount\": 169, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 84, \"title\": \"Post hoc analysis\", \"text\": \" in a scientific study , post hoc analysis ( from latin post hoc , \\\" after this \\\" ) consist of statistical analysis that be not specify before the datum be see . this typically create a multiple testing problem because each potential analysis be effectively a statistical test . multiple testing procedure be sometimes use to compensate , but that be often difficult or impossible to do precisely . post hoc analysis that be conduct and interpret without adequate consideration of this problem be sometimes call datum dredge by critic because the statistical association that it find be often spurious . \", \"tokencount\": 102, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 87, \"title\": \"Sequential pattern mining\", \"text\": \" sequential pattern mining be a topic of datum mining concern with find statistically relevant pattern between datum example where the value be deliver in a sequence . It be usually presume that the value be discrete , and thus time series mining be closely relate , but usually consider a different activity .   sequential pattern mining be a special case of structured datum mining . \", \"tokencount\": 66, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 87, \"title\": \"Sequential pattern mining\", \"text\": \" there be several key traditional computational problem address within this field .   these include build efficient database and index for sequence information , extract the frequently occur pattern , compare sequence for similarity , and recover miss sequence member .   in general , sequence mining problem can be classify as string mining which be typically base on string process algorithm   and itemset mining which be typically base on association rule learn . local process model   extend sequential pattern mine to more complex pattern that can include ( exclusive ) choice , loop , and concurrency construct in addition to the sequential ordering construct . \", \"tokencount\": 108, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 88, \"title\": \"Inauthentic text\", \"text\": \" an inauthentic text be a computer - generate expository document mean to appear as genuine , but which be actually meaningless .   frequently they be create in order to be intermix with genuine document and thus manipulate the result of search engine , as with spam blog .   They be also carry along in email in order to fool spam filter by give the spam the superficial characteristic of legitimate text . \", \"tokencount\": 74, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 88, \"title\": \"Inauthentic text\", \"text\": \" sometimes nonsensical document be create with computer assistance for humorous effect , as with dissociated press or flarf poetry .   They have also be use to challenge the veracity of a publication \\u2014 mit student submit paper generate by a computer program call scigen to a conference , where they be initially accept .   this lead the student to claim that the bar for submission be too low . \", \"tokencount\": 71, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 88, \"title\": \"Inauthentic text\", \"text\": \" with the amount of computer generate text outpace the ability of people to human to curate it , there need some mean of distinguish between the two .   yet automate approach to determine absolutely whether a text be authentic or not face intrinsic challenge of semantic .   noam chomsky coin the phrase \\\" colorless green idea sleep furiously \\\" give an example of grammatically - correct , but semantically incoherent sentence ; some will point out that in certain context one could give this sentence ( or any phrase ) meaning . \", \"tokencount\": 94, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 88, \"title\": \"Inauthentic text\", \"text\": \" the first group to use the expression in this regard can be find below from indiana university .   Their work explain in detail an attempt to detect inauthentic text and identify pernicious problem of inauthentic text in cyberspace .   the site have a means of submit text that assess , base on supervised learning , whether a corpus be inauthentic or not .   many user have submit incorrect type of datum and have correspondingly comment on the score . this application be mean for a specific kind of datum ; therefore , submit , say , an email , will not return a meaningful score . \", \"tokencount\": 109, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 90, \"title\": \"RapidMiner\", \"text\": \" rapidminer be a data science software platform develop by the company of the same name that provide an integrated environment for datum preparation , machine learning , deep learning , text mining , and predictive analytic . It be use for business and commercial application as well as for research , education , training , rapid prototyping , and application development and support all step of the machine learning process include datum preparation , result visualization , model validation and optimization . markus hofmann , ralf klinkenberg , \\\" rapidminer : data mining use cases and business analytics applications ( chapman & hall / crc data mining and knowledge discovery series ) , \\\" crc press , october 25 , 2013 . rapidminer be develop on an open core model . the rapidminer studio free edition , which be limit to 1 logical processor and 10,000 datum row be available under the agpl license.\\u201crapidminer , september 1 , 2015 . commercial pricing start at $ 5,000 and be available from the developer . \", \"tokencount\": 173, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 91, \"title\": \"HippoDraw\", \"text\": \" hippodraw be a powerful object - orient statistical datum analysis package write in c++ , with user interaction via a qt - base gui and a python - scriptable interface . It be be develop by paul kunz at slac , primarily for the analysis and presentation of particle physics and astrophysic datum , but can be equally well use in other field where datum handling be important . \", \"tokencount\": 69, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 91, \"title\": \"HippoDraw\", \"text\": \" hippodraw can read and write file in an xml - base format , astrophysic fits file , datum object produce by root ( optional ) , and through the python binding , anything that can be read / write by python ( hdf5 , for instance , with pytables ) . \", \"tokencount\": 51, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 91, \"title\": \"HippoDraw\", \"text\": \" hippodraw can be use as a python extension module , allow user to use hippodraw datum object with the full power of the python language . this include other scientific python extension modul such numeric and numarray , whose use with hippodraw can lead to a large increase in process speed , even for root object . \", \"tokencount\": 57, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 93, \"title\": \"Java Analysis Studio\", \"text\": \" java analysis studio ( jas ) be an object orient   datum analysis package develop for the analysis of particle physics datum . the late major version be jas3 . \", \"tokencount\": 30, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 93, \"title\": \"Java Analysis Studio\", \"text\": \" jas3 be particularly notable for be a fully aida - compliant datum analysis system . It be popular for datum analysis in area of particle physics which be familiar with the java programming language . \", \"tokencount\": 35, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 93, \"title\": \"Java Analysis Studio\", \"text\": \" the studio use many other library from the freehep project . \", \"tokencount\": 11, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 99, \"title\": \"Origin (data analysis software)\", \"text\": \" origin be a proprietary computer program for interactive scientific graphing and datum analysis . It be produce by originlab corporation , and run on microsoft windows . It have inspire several platform - independent open - source clone like scidavis . \", \"tokencount\": 41, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 99, \"title\": \"Origin (data analysis software)\", \"text\": \" datum analyse in origin include statistic , signal processing , curve fitting and peak analysis . origin 's curve fitting be perform by a nonlinear least square fitter which be base on the levenberg \\u2013 marquardt algorithm . \", \"tokencount\": 38, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 99, \"title\": \"Origin (data analysis software)\", \"text\": \" origin import data file in various format such as ascii text , excel , ni tdm , diadem , netcdf , spc , etc . It also export the graph to various image file format such as jpeg , gif , eps , tiff , etc . there be also a build - in query tool for access database datum via ado . \", \"tokencount\": 63, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 101, \"title\": \"Prior knowledge for pattern recognition\", \"text\": \" pattern recognition be a very active field of research intimately bind to machine learning . also know as classification or statistical classification , pattern recognition aim at build a classifier that can determine the class of an input pattern . this procedure , know as training , correspond to learn an unknown decision function base only on a set of input - output pair ( \\\\boldsymbol{x}_i , y_i ) that form the training datum ( or training set ) . nonetheless , in real world application such as character recognition , a certain amount of information on the problem be usually know beforehand . the incorporation of this prior knowledge into the training be the key element that will allow an increase of performance in many application . \", \"tokencount\": 128, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 105, \"title\": \"Grace Wahba\", \"text\": \" grace wahba ( bear august 3 , 1934 ) be a now - retire i. j. schoenberg - hilldale professor of statistics at the university of wisconsin \\u2013 madison . She be a pioneer in method for smooth noisy datum . best know for the development of generalize cross - validation and \\\" wahba 's problem \\\" , she have develop method with application in demographic study , machine learning , dna microarray , risk modeling , medical imaging , and climate prediction . \", \"tokencount\": 84, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 105, \"title\": \"Grace Wahba\", \"text\": \" She be educate at cornell ( b.a. 1956 ) , university of maryland , college park ( m.a. 1962 ) and stanford ( ph.d. 1966 ) , and work in industry for several year before receive her doctorate in 1966 and settle in madison in 1967 . She be the author of spline models for observational data . She be elect to the united states national academy of sciences in 2000 and receive an honorary degree of doctor of science from the university of chicago in 2007 . \", \"tokencount\": 88, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 108, \"title\": \"Rule induction\", \"text\": \" rule induction be an area of machine learn in which formal rule be extract from a set of observation .   the rule extract may represent a full scientific model of the datum , or merely represent local pattern in the datum . \", \"tokencount\": 43, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 109, \"title\": \"Information Harvesting\", \"text\": \" information harvesting ( ih ) be an early datum mining product from the 1990 .   It be invent by ralphe wiggins and produce by the ryan corp , later information harvesting inc. , of cambridge , massachusetts .   ih seek to infer rule from set of datum .   It do this first by classify various input variable into one of a number of bin , thereby put some structure on the continuous variable in the input .   ih then proceed to generate rule , trade off generalization against memorization , that will infer the value of the prediction variable , possibly create many level of rule in the process .   It include strategy for check if overfitt take place and , if so , correct for it .   because of its strategy for correct for overfitt by consider more datum , and refine the rule base on that datum , ih may also be consider to be a form of machine learning . \", \"tokencount\": 169, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 109, \"title\": \"Information Harvesting\", \"text\": \" the advantage of ih , as compare with other datum mining product of its time and even later , be that it provide a mechanism for find multiple rule that would classify the datum and determine , accord to set criterion , the good rule to use . \", \"tokencount\": 48, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 111, \"title\": \"Anomaly detection\", \"text\": \" in datum mining , anomaly detection ( also outlier detection ) be the identification of rare item , event or observation which raise suspicion by differ significantly from the majority of the datum . typically the anomalous item will translate to some kind of problem such as bank fraud , a structural defect , medical problem or error in a text . anomaly be also refer to as outlier , novelty , noise , deviation and exception . \", \"tokencount\": 78, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 111, \"title\": \"Anomaly detection\", \"text\": \" in particular , in the context of abuse and network intrusion detection , the interesting object be often not rare object , but unexpected burst in activity . this pattern do not adhere to the common statistical definition of an outlier as a rare object , and many outlier detection method ( in particular unsupervised method ) will fail on such datum , unless it have be aggregate appropriately . instead , a cluster analysis algorithm may be able to detect the micro cluster form by these pattern . \", \"tokencount\": 89, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 111, \"title\": \"Anomaly detection\", \"text\": \" three broad category of anomaly detection technique exist . unsupervised anomaly detection technique detect anomaly in an unlabeled test datum set under the assumption that the majority of the instance in the datum set be normal by look for instance that seem to fit least to the remainder of the datum set . supervised anomaly detection technique require a data set that have be label as \\\" normal \\\" and \\\" abnormal \\\" and involve train a classifier ( the key difference to many other statistical classification problem be the inherent unbalanced nature of outlier detection ) . semi - supervised anomaly detection technique construct a model represent normal behavior from a give normal training datum set , and then test the likelihood of a test instance to be generate by the learnt model . \", \"tokencount\": 135, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 112, \"title\": \"Minimum redundancy feature selection\", \"text\": \" minimum redundancy feature selection be an algorithm frequently use in a method to accurately identify characteristic of gene and phenotype and narrow down their relevance and be usually describe in its pairing with relevant feature selection as minimum redundancy maximum relevance ( mrmr ) . \\n feature selection , one of the basic problem in pattern recognition and machine learning , identify subset of datum that be relevant to the parameter use and be normally call maximum relevance . these subset often contain material which be relevant but redundant and mrmr attempt to address this problem by remove those redundant subset . mrmr have a variety of application in many area such as cancer diagnosis and speech recognition . \", \"tokencount\": 119, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 112, \"title\": \"Minimum redundancy feature selection\", \"text\": \" feature can be select in many different way . one scheme be to select feature that correlate strong to the classification variable . this have be call maximum - relevance selection . many heuristic algorithm can be use , such as the sequential forward , backward , or float selection . \", \"tokencount\": 51, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 112, \"title\": \"Minimum redundancy feature selection\", \"text\": \" on the other hand feature can be select to be mutually far away from each other while still have \\\" high \\\" correlation to the classification variable . this scheme , term as minimum redundancy maximum relevance ( mrmr ) selection have be find to be more powerful than the maximum relevance selection . \", \"tokencount\": 54, \"category\": \"Machine learning algorithms\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 112, \"title\": \"Minimum redundancy feature selection\", \"text\": \" as a special case , the \\\" correlation \\\" can be replace by the statistical dependency between variable . mutual information can be use to quantify the dependency . in this case , it be show that mrmr be an approximation to maximize the dependency between the joint distribution of the select feature and the classification variable . \", \"tokencount\": 58, \"category\": \"Machine learning algorithms\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 112, \"title\": \"Minimum redundancy feature selection\", \"text\": \" study have try different measure for redundancy and relevance measure . a recent study compare several measure within the context of biomedical image . auffarth , b. , lopez , m. , cerquides , j. ( 2010 ) . comparison of redundancy and relevance measure for feature selection in tissue classification of ct image . advance in data mining . application and theoretical aspects . p. 248 - -262 . springer . http://www.csc.kth.se/~auffarth/publications/redrel.pdf \", \"tokencount\": 73, \"category\": \"Machine learning algorithms\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 113, \"title\": \"Bayesian regret\", \"text\": \" in game theory , bayesian regret be the average difference between the utility of a strategy and an ideal utility where desire outcome be maximize . \\n the term bayesian refer to thomas bayes ( 1702\\u20131761 ) , who prove a special case of what be now call bayes ' theorem , who provide the first mathematical treatment of a non - trivial problem of statistical datum analysis use what be now know as bayesian inference . \", \"tokencount\": 77, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 115, \"title\": \"CN2 algorithm\", \"text\": \" the cn2 induction algorithm be a learn algorithm for rule induction . clark , p. and niblett , t ( 1989 ) the cn2 induction algorithm . machine learn 3(4):261 - 283 . It be design to work even when the training data be imperfect . It be base on idea from the aq algorithm and the id3 algorithm . as a consequence it create a rule set like that create by aq but be able to handle noisy datum like id3 . \", \"tokencount\": 83, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 118, \"title\": \"Shogun (toolbox)\", \"text\": \" shogun be a free , open - source machine learn software library   write in c++ . It offer numerous algorithm and datum structure for machine learning problem . It offer interface for octave , python , r , java , lua , ruby and c # use swig . \", \"tokencount\": 50, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 118, \"title\": \"Shogun (toolbox)\", \"text\": \" It be license under the term of the gnu general public license version 3 or later . \", \"tokencount\": 17, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 126, \"title\": \"Eager learning\", \"text\": \" in artificial intelligence , eager learning be a learning method in which the system try to construct a general , input - independent target function during training of the system , as oppose to lazy learning , where generalization beyond the training datum be delay until a query be make to the system . the main advantage gain in employ an eager learning method , such as an artificial neural network , be that the target function will be approximate globally during training , thus require much less space than use a lazy learning system . eager learning system also deal much good with noise in the training datum . eager learning be an example of offline learning , in which post - training query to the system have no effect on the system itself , and thus the same query to the system will always produce the same result . \", \"tokencount\": 151, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 126, \"title\": \"Eager learning\", \"text\": \" the main disadvantage with eager learning be that it be generally unable to provide good local approximation in the target function . \", \"tokencount\": 22, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 146, \"title\": \"Item tree analysis\", \"text\": \" item tree analysis ( ita ) be a data analytical method which allow construct ahierarchical structure on the item of a questionnaire or test from observed response \\n pattern . assume that we have a questionnaire with m item and that subject can \\n answer positive ( 1 ) or negative ( 0 ) to each of these item , i.e. the item be \\n dichotomous . if n subject answer the item this result in a binary datum matrix d \\n with m column and n row . \\n typical example of this datum format be test item which can be solve ( 1 ) or fail \\n ( 0 ) by subject . other typical example be questionnaire where the item be \\n statement to which subject can agree ( 1 ) or disagree ( 0 ) . \\n depend on the content of the item it be possible that the response of a subject to an \\n item j determine her or his response to other item . It be , for example , possible that \\n each subject who agree to item j will also agree to item i. in this case we say that \\n item j imply item i ( short i \\\\rightarrow j ) . the goal of an ita be to uncover such \\n deterministic implication from the datum set d. \", \"tokencount\": 229, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 149, \"title\": \"Photoanalysis\", \"text\": \" photoanalysis ( or photo analysis ) refer to the study of picture to compile various type of datum , for example , to measure the size distribution of virtually anything that can be capture by photo .   photoanalysis technology have change the way mine and mill quantify fragmented material . \\n images be a good way to document condition before , after , and even during blast activity .    the technology be advance at a high rate , and lens , storage medium memory , light sensitivity and resolution have be improve steadily .   today 's digital camera and camcorder include high - resolution optic , compact size , automatic time and date stamp , good battery life , shutter to freeze motion , and computer to autofocus and eliminate jitter use image stabilization . palangio , tom c. in the article \\\" digital image analysis \\\" featured in the journal of explosives engineering volume 26 , number 1 \", \"tokencount\": 162, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 152, \"title\": \"Topological data analysis\", \"text\": \" in applied mathematic , topological datum analysis ( tda ) be an approach to the analysis of dataset use technique from topology . extraction of information from dataset that be high - dimensional , incomplete and noisy be generally challenging . tda provide a general framework to analyze such datum in a manner that be insensitive to the particular metric choose and provide dimensionality reduction and robustness to noise .   beyond this , it inherit functoriality , a fundamental concept of modern mathematic , from its topological nature , which allow it to adapt to new mathematical tool . \", \"tokencount\": 100, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 152, \"title\": \"Topological data analysis\", \"text\": \" the initial motivation be to study the shape of datum . tda have combine algebraic topology and other tool from pure mathematic to allow mathematically rigorous study of \\\" shape \\\" . the main tool be persistent homology , an adaptation of homology to point cloud datum . persistent homology have be apply to many type of datum across many field . moreover , its mathematical foundation be also of theoretical importance . the unique feature of tda make it a promising bridge between topology and geometry . \", \"tokencount\": 88, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 153, \"title\": \"Combinatorial data analysis\", \"text\": \" in statistic , combinatorial datum analysis ( cda ) be the study of data set where the order in which object be arrange be important . cda can be use either to determine how well a give combinatorial construct reflect the observed datum , or to search for a suitable combinatorial construct that do fit the datum . \", \"tokencount\": 58, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 155, \"title\": \"PatientsLikeMe\", \"text\": \" patientslikeme be a for profit patient network and real - time research platform . through the network , patient connect with other who have the same disease or condition and track and share their own experience with the goal to improve outcome . in the process , they generate datum about the real - world nature of disease . with over 600,000 member , patientslikeme be a source for real - world disease information and its patient - generate datum form the basis of more than 100 peer - review scientific study . patientslikeme be inspire by the life experience of stephen heywood , diagnose in 1998 at the age of 29 with amyotrophic lateral sclerosis ( als ) , or lou gehrig 's disease . the company be found in 2004 by his brother jamie and ben heywood and long - time family friend jeff cole . \", \"tokencount\": 148, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 158, \"title\": \"Kirix Strata\", \"text\": \" kirix strata be a discontinue specialty web browser design for datum analytic .   strata offer a browser 's ability to view web page , but also include additional tool to perform datum analysis and create report base on structured datum from local file , external relational database and the web . \", \"tokencount\": 52, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 158, \"title\": \"Kirix Strata\", \"text\": \" the browser incorporate mozilla'smozilla - base applications xulrunnermozilla xulrunner hall of fame into a proprietary data engine to create a rich internet application for work with datum . kirix strata be a commercial product that use the cross - platform wxwidget toolkit and be support on both microsoft windows and linux . \", \"tokencount\": 52, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 159, \"title\": \"Uncertain data\", \"text\": \" in computer science , uncertain datum be datum that contain noise that make it deviate from the correct , intended or original value . in the age of big datum , uncertainty or datum veracity be one of the defining characteristic of datum . datum be constantly grow in volume , variety , velocity and uncertainty ( 1/veracity ) . uncertain data be find in abundance today on the web , in sensor network , within enterprise both in their structured and unstructured source . for example , there may be uncertainty regard the address of a customer in an enterprise dataset , or the temperature reading capture by a sensor due to age of the sensor . in 2012 ibm call out manage uncertain datum at scale in its global technology outlook report that present a comprehensive analysis look three to ten year into the future seek to identify significant , disruptive technology that will change the world . in order to make confident business decision base on real - world datum , analysis must necessarily account for many different kind of uncertainty present in very large amount of datum . analysis base on uncertain datum will have an effect on the quality of subsequent decision , so the degree and type of inaccuracy in this uncertain datum can not be ignore . \\n uncertain data be find in the area of sensor network ; text where noisy text be find in abundance on social medium , web and within enterprise where the structured and unstructured datum may be old , outdated , or plain incorrect ; in model where the mathematical model may only be an approximation of the actual process . when represent such datum in a database , some indication of the probability of the correctness of the various value also need to be estimate . \", \"tokencount\": 310, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 8.182528469706376}, {\"docid\": 159, \"title\": \"Uncertain data\", \"text\": \" there be three main model of uncertain datum in database . in attribute uncertainty , each uncertain attribute in a tuple be subject to its own independent probability distribution . for example , if reading be take of temperature and wind speed , each would be describe by its own probability distribution , as know the reading for one measurement would not provide any information about the other . \", \"tokencount\": 69, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 159, \"title\": \"Uncertain data\", \"text\": \" in correlated uncertainty , multiple attribute may be describe by a joint probability distribution . for example , if reading be take of the position of an object , and the x- and y - coordinate store , the probability of different value may depend on the distance from the record coordinate . as distance depend on both coordinate , it may be appropriate to use a joint distribution for these coordinate , as they be not independent . \", \"tokencount\": 79, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 159, \"title\": \"Uncertain data\", \"text\": \" in tuple uncertainty , all the attribute of a tuple be subject to a joint probability distribution . this cover the case of correlated uncertainty , but also include the case where there be a probability of a tuple not belong in the relevant relation , which be indicate by all the probability not sum to one . for example , assume we have the follow tuple from a probabilistic database : \\n  ( a , 0.4 ) | ( b , 0.5 ) \", \"tokencount\": 84, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 159, \"title\": \"Uncertain data\", \"text\": \" then , the tuple have 10 % chance of not exist in the database . \", \"tokencount\": 15, \"category\": \"Machine learning\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 160, \"title\": \"Double mass analysis\", \"text\": \" double mass analysis searcy , j.k. and c.h. hardison ( 1960 ) . double - mass curve . u.s. geological survey water - supply paper 1541-b '' '' be a commonly use datum analysis approach for investigate the behaviour of record make of hydrological or meteorological datum at a number of location . It be use to determine whether there be a need for correction to the datum - to account for change in datum collection procedure or other local condition . such change may result from a variety of thing include change in instrumentation , change in observation procedure , or change in gauge location or surround condition . double mass analysis for check consistency of a hydrological or meteorological record be consider to be an essential tool before take it for analysis purpose . this method be base on the hypothesis that each item of the record datum of a population be consistent . \\n an example of a double mass analysis be a \\\" double mass plot \\\" , or \\\" double mass curve\\\".wilson , e.m. ( 1983 ) engineering hydrology , 3rd edition . macmillan press , london . p.27 for this , point and/or a join line be plot where the x- and y- coordinate be determine by the run total of the value observe at two station . if both station be affect to the same extent by the same trend then a double mass curve should follow a straight line . a break in the slope of the curve would indicate that condition have change at one location but not at another . \\n this technique be base on the principle that when each record datum come from the same parent population , they be consistent . \", \"tokencount\": 294, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 162, \"title\": \"Tableau Software\", \"text\": \" tableau software ( ) be an interactive data visualization software companya dead - simple tool that let anyone create interactive maps found on january 2003 by christian chabot , pat hanrahan and chris stolte , in mountain view , california . the company be currently headquarter in seattle , washington , united states tableau software help datum become more visual focus on business intelligence . on june 10 , 2019 ,   salesforce.com announce it would be acquire tableau . \", \"tokencount\": 80, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 162, \"title\": \"Tableau Software\", \"text\": \" chabot , hanrahan and stolte be researcher at the department of computer science at stanford university   who specialize in visualization technique for explore and analyze relational database and datum cube . the company be start as a commercial outlet for research produce at stanford between 1999 - 2002 . \", \"tokencount\": 50, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 162, \"title\": \"Tableau Software\", \"text\": \" tableau product query relational database , online analytical processing cube , cloud database , and spreadsheet to generate graph - type datum visualization . the product can also extract , store , and retrieve datum from an in - memory datum engine . \", \"tokencount\": 43, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 165, \"title\": \"Ocean Data View\", \"text\": \" ocean data view ( odv ) be a proprietary , freely available , software package for the analysis and visualization of oceanographic and meteorological data set . reiner schlitzer , interactive analysis and visualization of geoscience datum with ocean data view , computers & geosciences , volume 28 , issue 10 , december 2002 , pages 1211 - 1218 , http://epic.awi.de/publications/sch2001h.pdfhttp://gcmd.nasa.gov/records/odv_awi.htmlhttp://www.tos.org/oceanography/issues/issue_archive/.../11.2_brown.pdf \\n odv be use by a large number of oceanographer .   the unesco ocean teacher project employ odv as one of its main analysis and display tool . odv homepage   odv be use to display and analyze datum from several oceanographic project such as argo , world ocean circulation experiment , world ocean database project , seadatanet , world ocean atlas , and medar / medatlas project . murray brown , ocean data view 4.0 , oceanography , 11 , no 2/1998 , 19 - 21 , http://www.jcommops.org/ftproot/argo/tools/odv/mbrown_odv_review.pdf \\n ocean data view include also option that permit to perform objective analysis thank to the add - on diva software . \", \"tokencount\": 173, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 167, \"title\": \"SPSS Modeler\", \"text\": \" ibm spss modeler be a data mining and text analytic software application from ibm . It be use to build predictive model and conduct other analytic task . It have a visual interface which allow user to leverage statistical and datum mining algorithm without programming . one of its main aim from the outset be to get rid of unnecessary complexity in datum transformation , and to make complex predictive model very easy to use . the first version incorporate decision tree ( id3 ) , and neural network ( backprop ) , which could both be train without underlie knowledge of how those technique work . \", \"tokencount\": 107, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 167, \"title\": \"SPSS Modeler\", \"text\": \" ibm spss modeler be originally name clementine by its creator , integral solutions limited . this name continue for a while after spss 's acquisition of the product . spss later change the name to spss clementine , and then later to pasw modeler . follow ibm 's 2009 acquisition of spss , the product be rename ibm spss modeler , its current name . \", \"tokencount\": 65, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 168, \"title\": \"OpenScientist\", \"text\": \" openscientist be an integration of open source product work together to do scientific visualization and datum analysis , in particular for high energy physics ( hep ) . \", \"tokencount\": 28, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 168, \"title\": \"OpenScientist\", \"text\": \" among other thing , it contain a light c++ aida implementation that can be use to run the histogramming part of geant4 example . \", \"tokencount\": 24, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 170, \"title\": \"PowerLab\", \"text\": \" powerlab ( before 1998 be refer to as maclab ) be a data acquisition system develop by adinstrument comprise hardware and software and design for use in life science researchpaper on google scholar reference to adinstruments & powerlab and teaching application . It be commonly use in physiology , pharmacology , biomedical engineering , sport / exercise study and psychophysiology laboratory to record and analyse physiological signal from human or animal subject or from isolated organ . the system consist of an input device connect to a microsoft windows or mac os computer use a usb cable and labchart software which be supply with the powerlab and provide the recording , display and analysis function . the use of powerlab and supplementary adinstruments product have be demonstrate on the journal of visualised experiments . jove : journal of visualised experiments use powerlab \\n the original maclab unit be develop in the late 1980 to run with only macintosh computer to perform computer - base datum acquisition and analysis . the maclab product range be rename \\\" powerlab \\\" in 1997 to reflect the cross - platform nature of the system . \", \"tokencount\": 191, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 170, \"title\": \"PowerLab\", \"text\": \" the powerlab system be essentially a peripheral device design to perform various function need for datum acquisition , signal conditioning and pre - processing .   versatile display option and analysis function be complement by the ability to export datum to other software ( such as microsoft excel ) . \", \"tokencount\": 50, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" xlfit be a microsoft excel - base plug - in which perform regression , curve fitting and statistical analysis . xlfit generate 2d and 3d graph and analyse data set produce by any type of research . xlfit \\u2019s curve fit engine allow linear and non - linear curve fit , smoothing , statistic , weighting and error bar . \", \"tokencount\": 60, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" xlfit include over seventy linear and non - linear curve fitting model . predefined category include : \\n  exponential / log , power series , sigmoidal , hyperbolic , yield density , linear , polynomial , dose response , pharmacology , equilibrium , inhibition \\n  levenberg - marquardt fitting algorithm \", \"tokencount\": 50, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" a range of statistical calculation can also be apply to the datum from within the spreadsheet . \", \"tokencount\": 17, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" example statistic include : \\n  f test and t test \\n  area under the curve \\n  confidence interval \\n  error value relate to parameter value or any point on the curve . \", \"tokencount\": 32, \"category\": \"Data analysis software\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" available statistic model include \\n spearman 's rank correlations , student 's t - test , mann whitney u - test , least square , anova , among other . It be possible to add your own model and statistic to the provide one use a model editor . \", \"tokencount\": 49, \"category\": \"Data analysis software\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" xlfit be validate by the uk national physical laboratoryhttp://www.engineeringtalk.com / news / ade / ade261.html in 2004 . the unit test for this be provide in the model editor from version 5.4 onwards to allow each version to be easily validate . \", \"tokencount\": 42, \"category\": \"Data analysis software\", \"lineid\": 5, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 172, \"title\": \"XLfit\", \"text\": \" xlfit be use by nasa to analyse the battery life of the curiosity mars lander . \", \"tokencount\": 16, \"category\": \"Data analysis software\", \"lineid\": 6, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 174, \"title\": \"Data Mining and Knowledge Discovery\", \"text\": \" data mining and knowledge discovery be a bimonthly peer - review scientific journal focus on datum mining publish by springer science+business media .   It be start in 1996 and launch in 1997 by usama fayyad as found editor - in - chief by kluwer academic publishers ( later become springer ) . the first editorial provide a summary of why it be start . \", \"tokencount\": 65, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 175, \"title\": \"Instance-based learning\", \"text\": \" in machine learning , instance - base learning ( sometimes call memory - base learning ) be a family of learn algorithm that , instead of perform explicit generalization , compare new problem instance with instance see in training , which have be store in memory . \\n It be call instance - base because it construct hypothesis directly from the training instance themselves . stuart russell and peter norvig ( 2003 ) . artificial intelligence : a modern approach , second edition , p. 733 . prentice hall . \\n this mean that the hypothesis complexity can grow with the datum : in the bad case , a hypothesis be a list of n train item and the computational complexity of classify a single new instance be o(n ) . one advantage that instance - base learning have over other method of machine learning be its ability to adapt its model to previously unseen datum . instance - base learner may simply store a new instance or throw an old instance away . \", \"tokencount\": 174, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 175, \"title\": \"Instance-based learning\", \"text\": \" example of instance - base learning algorithm be the k - near neighbor algorithm , kernel machine and rbf network . these store ( a subset of ) their training set ; when predict a value / class for a new instance , they compute distance or similarity between this instance and the training instance to make a decision . \", \"tokencount\": 60, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 175, \"title\": \"Instance-based learning\", \"text\": \" to battle the memory complexity of store all training instance , as well as the risk of overfitt to noise in the training set , instance reduction algorithm have be propose . \", \"tokencount\": 32, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 175, \"title\": \"Instance-based learning\", \"text\": \" gagliardi apply this family of classifier in medical field as second - opinion diagnostic tool and as tool for the knowledge extraction phase in the process of knowledge discovery in database . \\n one of these classifier ( call prototype exemplar learn classifier ( pel - c ) be able to extract a mixture of abstract prototypical case ( that be syndrome ) and select atypical clinical case . \", \"tokencount\": 69, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 177, \"title\": \"Biopac student lab\", \"text\": \" the biopac student lab be a proprietary teaching device and method introduce in 1995 as a digital replacement for age chart recorder and oscilloscope that be widely use in undergraduate teaching laboratory prior to that time . It be manufacture by biopac systems , inc. , of goleta , california.biopac the advent of low cost personal computer mean that old analog technology could be replace with powerful and less expensive computerized alternative . investigative process & technology in introductory physiology    author : hawke , scott d. \", \"tokencount\": 87, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 177, \"title\": \"Biopac student lab\", \"text\": \" student in undergraduate teaching lab use the bsl system to record datum from their own body , animal or tissue preparation . the bsl system integrate hardware , software and curriculum material include over sixty experiment that student use to study the cardiovascular system , muscle , pulmonary function , autonomic nervous system , and the brain . \", \"tokencount\": 58, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 179, \"title\": \"L\\u00e9on Bottou\", \"text\": \" l\\u00e9on bottou ( bear 1965 ) be a researcher best know for his work in machine learning and datum compression . His work present stochastic gradient descent as a fundamental learning algorithm . He be also one of the main creator of the djvu image compression technology ( together with yann lecun and patrick haffner ) , and the maintainer of djvulibre , the open source implementation of djvu . He be the original developer of the lush programming language . \", \"tokencount\": 81, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 181, \"title\": \"Kunihiko Fukushima\", \"text\": \" kunihiko fukushima ( bear in japan ) be a japanese computer scientist , most note for his work on artificial neural network and deep learning . He be currently work part - time as a senior research scientist at the fuzzy logic systems institute in tokyo . \", \"tokencount\": 47, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 181, \"title\": \"Kunihiko Fukushima\", \"text\": \" in 1980 , fukushima publish the neocognitron , \\n the original deep convolutional neural network ( cnn ) architecture . fukushima propose several supervised and unsupervised learn algorithm to train the parameter of a deep neocognitron such that it could learn internal representation of incoming datum . ( today , however , the cnn architecture be usually train through backpropagation . this approach be now heavily use in computer vision . ) \", \"tokencount\": 72, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 181, \"title\": \"Kunihiko Fukushima\", \"text\": \" in 1958 , fukushima receive his bachelor of engineering in electronic from kyoto university . He become a senior research scientist at the nhk science & technology research laboratories . in 1989 , he join the faculty of osaka university . in 1999 , he join the faculty of the university of electro - communications . in 2001 , he join the faculty of tokyo university of technology . from 2006 to 2010 , he be a visit professor at kansai university . \", \"tokencount\": 83, \"category\": \"Machine learning researchers\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 181, \"title\": \"Kunihiko Fukushima\", \"text\": \" fukushima act as found president the japanese neural network society ( jnns ) . He also be a found member on the board of governors of the international neural network society ( inns ) , and president of the asia - pacific neural network assembly ( apnna ) . \", \"tokencount\": 49, \"category\": \"Machine learning researchers\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 186, \"title\": \"MEX file\", \"text\": \" a mex file be a type of computer file that provide an interface between matlab or octave and function write in c , c++ or fortran . It stand for \\\" matlab executable \\\" . \\n when compile , mex file be dynamically load and allow external function to be invoke from within matlab or octave as if they be build - in function . \", \"tokencount\": 65, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 186, \"title\": \"MEX file\", \"text\": \" to support the development of mex file , both matlab and octave offer external interface function that facilitate the transfer of datum between mex file and the workspace . in addition to mex file , octave have its own format use its own native api , with good performance.https://www.gnu.org/software/octave/doc/interpreter/mex_002dfiles.html \", \"tokencount\": 49, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" speakeasy be a numerical compute interactive environment also feature an interpreted programming language . It be initially develop for internal use at the physics division of argonne national laboratory by the theoretical physicist stanley cohen.\\\"an introduction to speakeasy - informal report He eventually found speakeasy computing corporation to make the program available commercially . \", \"tokencount\": 54, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" speakeasy be a very long - last numerical package . in fact , the original version of the environment be build around a core dynamic datum repository call \\\" name storage \\\" develop in the early 1960s,\\\"named storage : a dynamic storage - allocation scheme with manipulative routine \\\" , aec research and development report - volume 7021 anl ( series ) - stanley cohen , physics division , u.s. atomic energy commission , argonne national laboratory , 1965.\\\"speakeasy - an evolutionary system \\\" , s. cohen ,   proceedings of the acm sigplan symposium on very high level language ( march 1974 ) while the most recent version have be release in 2006 . \", \"tokencount\": 115, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" speakeasy be aim to make the computational work of the physicist at the argonne national laboratory easier . It be initially conceive to work on mainframe ( the only kind of computer at that time ) , and be subsequently port to new platform ( minicomputer , personal computer ) as they become available . the porting of the same code on different platform be make easy by use mortran metalanguage macro to face system dependency and compiler deficiency and differences.\\\"us mortran to translate fortran program from one machine to another \\\" \\n steven c. pieper , argonne national laboratory , 1976 speakeasy be currently available on several platform : pc run windows , macos , linux , departmental computer and workstation run several flavor of linux , aix or solaris . \", \"tokencount\": 132, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" speakeasy be also among the first interactive numerical computing environment , have be implement in such a way on a cdc 3600 system , and later on ibm tso machine as one be in beta - testing at the argonne national laboratory at the time . \", \"tokencount\": 46, \"category\": \"Data analysis software\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" almost since the beginning ( as the dynamic linking functionality be make available in the operating system ) speakeasy feature the capability of expand its operational vocabulary use separate module , dynamically link to the core processor as they be need . for that reason such module   be call \\\" linkule \\\" ( linkable - modules).\\\"speakeasy linkule - plug compatible software \\\" acm - proceeding of the 1977 annual conference They be function with a generalize interface , which can be write in   fortran or in c. \\n the independence of each of the new module from the other and from the main processor be of great help in improve the system , especially it be in the old day . \", \"tokencount\": 123, \"category\": \"Data analysis software\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" this easy way of expand the functionality of the main processor be often exploit by the user to develop their own specialized package . besides the program , function and subroutine the user can write in the speakeasy 's own interpreted language , linkule add functionality carry out with the typical performance of compile program . \", \"tokencount\": 56, \"category\": \"Data analysis software\", \"lineid\": 5, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 187, \"title\": \"Speakeasy (computational environment)\", \"text\": \" among the package develop by the user , one of the most important be \\\" modeleasy \\\" , originally develop as \\\" fedeasy\\\"\\\"econometric model via speakeasy / fedeasy \\\" , james m. condie , john w. davison , 1975 in the early 1970 at the research department of the federal reserve board of governors in washington d.c .. \\n modeleasy implement special object and function for large econometric model estimation and simulation . \\n Its evolution lead eventually to its distribution as an independent product . \", \"tokencount\": 86, \"category\": \"Data analysis software\", \"lineid\": 6, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 189, \"title\": \"KNIME\", \"text\": \" knime ( ) , the konstanz information miner , be a free and open - source data analytic , reporting and integration platform . knime integrate various component for machine learning and datum mining through its modular datum pipelin concept . a graphical user interface and use of jdbc allow assembly of node blend different datum source , include preprocessing ( etl : extraction , transformation , loading ) , for modeling , datum analysis and visualization without , or with only minimal , programming . to some extent as advanced analytic tool knime can be consider as a sas alternative . \", \"tokencount\": 102, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 189, \"title\": \"KNIME\", \"text\": \" since 2006 , knime have be use in pharmaceutical research , it also use in other area like crm customer datum analysis , business intelligence and financial datum analysis . \", \"tokencount\": 30, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 190, \"title\": \"Learning to rank\", \"text\": \" learn to rank . slide from tie - yan liu 's talk at www 2009 conference be available online \\n  or machine - learn ranking ( mlr ) be the application of machine learning , typically supervised , semi - supervised or reinforcement learning , in the construction of ranking model for information retrieval system . mehryar mohri , afshin rostamizadeh , ameet talwalkar ( 2012 ) foundation of machine learning , the \\n mit press . training datum consist of list of item with some partial order specify between item in each list . this order be typically induce by give a numerical or ordinal score or a binary judgment ( e.g. \\\" relevant \\\" or \\\" not relevant \\\" ) for each item . the ranking model 's purpose be to rank , i.e. produce a permutation of item in new , unseen list in a way which be \\\" similar \\\" to ranking in the training datum in some sense . \", \"tokencount\": 164, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 192, \"title\": \"Encog\", \"text\": \" encog be a machine learn framework available for java and .net . j. heaton http://www.jmlr.org/papers/volume16/heaton15a/heaton15a.pdf encog : library of interchangeable machine learning models for java and c # \\n encog support different learning algorithm such as   bayesian networks , hidden markov models and support vector machines . \\n however , its main strength lie in its neural network algorithm . encog contain class to create a wide variety of network , as well as support class to normalize and process datum for these neural network . encog train use many different technique .   multithreading be use to allow optimal training performance on multicore machine . \", \"tokencount\": 107, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 192, \"title\": \"Encog\", \"text\": \" encog can be use for many task , include medicald. heider , j. verheyen , d. hoffmann http://www.biomedcentral.com/content/pdf/1471-2105-11-37.pdf predicting bevirimat resistance of hiv-1 from genotype and financial research . j. heaton http://www.devx.com/opensource/article/44014/1954 basic market forecasting with encog neural networks a gui base workbench be also provide to help model and train neural network .   encog have be in active development since 2008.http://www.heatonresearch.com/encog description of encog project . \", \"tokencount\": 68, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 193, \"title\": \"PVLV\", \"text\": \" the primary value learn value ( pvlv ) model be a possible explanation for the reward - predictive firing property of dopamine ( da ) neuron . It simulate behavioral and neural datum on pavlovian conditioning and the midbrain dopaminergic neuron that fire in proportion to unexpected reward . It be an alternative to the temporal - difference ( td ) algorithm . \\n It be use as part of leabra . \", \"tokencount\": 72, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 200, \"title\": \"ELKI\", \"text\": \" elki ( for environment for developing kdd - applications support by index - structures ) be a knowledge discovery in database ( kdd , \\\" datum mining \\\" ) software framework develop for use in research and teaching originally at the database system research unit of professor hans - peter kriegel at the ludwig maximilian university of munich , germany . It aim at allow the development and evaluation of advanced datum mining algorithm and their interaction with database index structure . \", \"tokencount\": 82, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 201, \"title\": \"Local outlier factor\", \"text\": \" in anomaly detection , the local outlier factor ( lof ) be an algorithm propose by markus m. breunig , hans - peter kriegel , raymond t. ng and j\\u00f6rg sander in 2000 for find anomalous datum point by measure the local deviation of a give data point with respect to its neighbour . \", \"tokencount\": 54, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 201, \"title\": \"Local outlier factor\", \"text\": \" lof share some concept with dbscan and optics such as the concept of \\\" core distance \\\" and \\\" reachability distance \\\" , which be use for local density estimation . \", \"tokencount\": 31, \"category\": \"Data mining\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" hans - peter kriegel ( 1 october 1948 , germany ) be a german computer scientist and professor at the ludwig maximilian university of munich and lead the database systems group in the department of computer science . \", \"tokencount\": 38, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" His most important contribution be the database index structure r*-tree , x - tree and iq - tree , the cluster analysis algorithm   dbscan , optics and subclu and the anomaly detection method local outlier factor ( lof ) . \", \"tokencount\": 41, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" in 2009 the association for computing machinery appoint hans - peter kriegel a \\\" fellow \\\" , one of its high honor . He have be honor in particular for his contribution to \\\" knowledge discovery and datum mining , similarity search , spatial datum management , and access method for high - dimensional datum \\\" . \", \"tokencount\": 57, \"category\": \"Machine learning researchers\", \"lineid\": 2, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" He receive the 2013 ieee icdm research contributions award for his research on datum mining algorithm such as dbscan , optics , local outlier factor and his work on mine high - dimensional datum . \", \"tokencount\": 35, \"category\": \"Machine learning researchers\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" He be also award the 2015 acm sigkdd innovation award for his contribution to datum mining in clustering , outlier detection and high - dimensional datum analysis , in particular for density - base approach . \\n dbscan also receive the 2014 acm sigkdd test of time award . \", \"tokencount\": 49, \"category\": \"Machine learning researchers\", \"lineid\": 4, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" He be the most cite german researcher in database and datum mining . \", \"tokencount\": 13, \"category\": \"Machine learning researchers\", \"lineid\": 5, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" His current research be focus around correlation clustering , high - dimensional datum indexing and analysis , spatial datum mining and spatial datum management as well as multimedia database . \", \"tokencount\": 30, \"category\": \"Machine learning researchers\", \"lineid\": 6, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 202, \"title\": \"Hans-Peter Kriegel\", \"text\": \" His research group publish a java software framework title environment for developing kdd - applications support by index - structures ( elki ) that be design for the parallel research of index structure , datum mining algorithm and their interaction , such as optimize data mining algorithm base on database index . \", \"tokencount\": 52, \"category\": \"Machine learning researchers\", \"lineid\": 7, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 203, \"title\": \"Arnetminer\", \"text\": \" arnetminer ( also aminer ) be a free online service use to index , search , and mine big scientific datum . \", \"tokencount\": 22, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 204, \"title\": \"CANalyzer\", \"text\": \" canalyzer be an analysis software tool from vector informatik gmbh. this development software be widely use , primarily by automotive and electronic control unit supplier , to analyze the datum traffic in serial bus system . the most relevant bus system here be can , lin , flexray , ethernet and mostcanalyzer website , download november 3rd , 2011 as well as can - base protocol such as j1939,canalyzer . j1939 , download november 3rd , 2011 canopen , canopen solution , download november 3rd , 2011 arinc 825overview can - base avionic protocol on www.avionics-networking.com , download   june 17th , 2010 and many more . \", \"tokencount\": 107, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 205, \"title\": \"Elastic map\", \"text\": \" elastic map provide a tool for nonlinear dimensionality reduction . by their construction , they be a system of elastic spring   embed in the datum \\n space . this system approximate a low - dimensional manifold . the elastic coefficient of this system allow the switch from completely unstructured k - means clustering ( zero elasticity ) to the estimator locate closely to linear pca manifold ( for high bend and low stretching module ) . with some intermediate value of the elasticity coefficient , this system effectively approximate non - linear principal manifold . this approach be base on a mechanical analogy between principal manifold , that be pass through \\\" the middle \\\" of the data distribution , and elastic membrane and plate . the method be develop by a.n. gorban , a.y. zinovyev and a.a. pitenko in 1996\\u20131998 . \", \"tokencount\": 143, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 216, \"title\": \"Multilinear subspace learning\", \"text\": \" multilinear subspace learning be an approach to dimensionality reduction . m. a. o. vasilescu , d. terzopoulos ( 2003 ) \\\" multilinear subspace analysis of image ensembles \\\" , \\\" proceeding of the ieee conference on computer vision and pattern recognition ( cvpr\\u201903 ) , madison , wi , june , 2003\\\"m. a. o. vasilescu , d. terzopoulos ( 2002 ) \\\" multilinear analysis of image ensembles : tensorfaces \\\" ,   proc . 7th european conference on computer vision ( eccv'02 ) , copenhagen , denmark , may , 2002m. a. o. vasilescu,(2002 ) \\\" human motion signatures : analysis , synthesis , recognition \\\" , \\\" proceeding of international conference on pattern recognition ( icpr 2002 ) , vol . 3 , quebec city , canada , aug , 2002 , 456\\u2013460 . \\\"   \\n dimensionality reduction can be perform on a data tensor whose observation have be vectoriz and organize into a data tensor , or whose observation be matrix that be concatenate into a data tensor . x. he , d. cai , p. niyogi , tensor subspace analysis , in : advance in neural information processing systemsc 18 ( nips ) , 2005 .   here be some example of datum tensor whose observation be vectoriz   or whose observation be matrix concatenate into datum tensor image ( 2d/3d ) , video sequence ( 3d/4d ) , and hyperspectral cube ( 3d/4d ) . \", \"tokencount\": 239, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 216, \"title\": \"Multilinear subspace learning\", \"text\": \" the mapping from a high - dimensional vector space to a set of low dimensional vector space be a multilinear projection . \", \"tokencount\": 22, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 216, \"title\": \"Multilinear subspace learning\", \"text\": \" multilinear subspace learn algorithm be high - order generalization of linear subspace learning method such as principal component analysis ( pca ) , independent component analysis ( ica ) , linear discriminant analysis ( lda ) and canonical correlation analysis ( cca ) . \", \"tokencount\": 44, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 217, \"title\": \"Rexer's Annual Data Miner Survey\", \"text\": \" rexer analytics \\u2019s annual data miner survey be the large survey of datum mining , data science , and analytic professional in the industry . It consist of approximately 50 multiple choice and open - end question that cover seven general area of datum mining science and practice :   ( 1 ) field and goal , ( 2 ) algorithms , ( 3 ) model , ( 4 ) tool ( software package use ) , ( 5 ) technology , ( 6 ) challenge , and ( 7 ) future .   It be conduct as a service ( without corporate sponsorship ) to the datum mining community , and the result be usually announce at the paw ( predictive analytics world ) conference and share via freely available summary report .   in the 2013 survey , 1259 data miner from 75 country participate . karl rexer , heather allen , & paul gearan ( 2011 ) ; 2011 data miner survey summary '' , present at predictive analytics world , October 2011 .   after 2011 , rexer analytics move to a biannual schedule . \", \"tokencount\": 189, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 219, \"title\": \"DataScene\", \"text\": \" datascene be a scientific graphing , animation , datum analysis , and real - time datum monitor software package . m. bedford , \\\" how to create high - quality graphs and charts \\\" , computer shopper magazine , p. 135 , issue 273 ( 2010).l. g. rubin . \\\" focus on software : graphing software \\\" , physics today , p. 62 , 62(7 ) ( 2009 ) It be develop with the common language infrastructure technology and the gdi+ graphic library . with the two common language runtime engine - the .net and mono framework - datascene run on all major operating system . \", \"tokencount\": 106, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 219, \"title\": \"DataScene\", \"text\": \" with datascene , the user can plot 39 type 2d & 3d graph ( e.g. , area graph , bar graph , boxplot graph , pie graph , line graph , histogram graph , surface graph , polar graph , water fall graph , etc . ) , manipulate , print , and export graph to various format ( e.g. , bitmap , wmf / emf , jpeg , png , gif , tiff , postscript , and pdf ) , analyze datum with different mathematical method ( fitting curve , calculate static , fft , etc . ) , create chart animation for presentation ( e.g. with powerpoint ) , class , and web page , and monitor and chart real - time datum . \", \"tokencount\": 126, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 222, \"title\": \"Moose (analysis)\", \"text\": \" moose be a free and open source platform for software and datum analysis build in pharo . \", \"tokencount\": 17, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 222, \"title\": \"Moose (analysis)\", \"text\": \" moose offer multiple service range from import and parse datum , to modeling , to measure , querying , mining , and to build interactive and visual analysis tool . moose be bear in a research context , oscar nierstrasz , st\\u00e9phane ducasse , and tudor g\\u00eerba . the story of moose : an agile reengineering environment . in proceeding of the european software engineering conference ( esec / fse'05 ) , p. 1\\u201410 , acm press , new york ny , 2005 . invited paper . and it be currently support by several research group throughout the world . It be increasingly be adopt in industry . \", \"tokencount\": 108, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 223, \"title\": \"EgoNet\", \"text\": \" egonet ( egocentric network study software ) for the collection and analysis of egocentric social network data.http://egonet.softpedia.com/ download egonet free It help the user to collect and analyse all the egocentric network datum ( all social network datum of a website on the internet ) , and provide general global network measure and datum matrix that can be use for further analysis by other software . the egonet be the result of the link that it give and receive certain address on the internet , http://www.vidadigital.net / blog/2005/11/09/egonet - y - rede - sociales/ egonet and social network - digizen and egonet be dedicate to collect information about them and present it in a way useful to the user . \", \"tokencount\": 120, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 223, \"title\": \"EgoNet\", \"text\": \" egonet be write in java , so that the computer where it be go to be use must have the jre instal . egonet be open source software , license under gpl.http://sourceforge.net / project / egonet/ egonet - download egonet software for free at sourceforge.net \", \"tokencount\": 45, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 223, \"title\": \"EgoNet\", \"text\": \" Its creator be professor christopher mccarty , of the university of florida , united states . \\n http://escoladeredes.ning.com/profiles/blogs/egonet-1 egonet - escola de redes \", \"tokencount\": 23, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 224, \"title\": \"Ball tree\", \"text\": \" in computer science , a ball tree , balltree or metric tree , be a space partition datum structure for organize point in a multi - dimensional space . the ball tree get its name from the fact that it partition data point into a nest set of hypersphere know as \\\" ball \\\" . the result data structure have characteristic that make it useful for a number of application , most notably near neighbor search . \", \"tokencount\": 77, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 227, \"title\": \"Sepp Hochreiter\", \"text\": \" sepp hochreiter ( bear josef hochreiter in 1967 ) be a german computer scientist . since 2018 he be head the institute for machine learning at the johannes kepler university of linz after have lead the institute of bioinformatics from 2006 to 2018 . since 2017 he be also head of the linz institute of technology ( lit ) ai lab which focus on advance research on artificial intelligence . previously , he be at the technical university of berlin , at the university of colorado at boulder , and at the technical university of munich . \", \"tokencount\": 97, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 227, \"title\": \"Sepp Hochreiter\", \"text\": \" sepp hochreiter have make numerous contribution in the field of machine learning , deep learning and bioinformatic . He develop the long short - term memory ( lstm ) for which the first result be report in his diploma thesis in 1991 . the main lstm paper appear in 1997 and be consider as a discovery that be a milestone in the timeline of machine learning . the foundation of deep learning be lead by his analysis of the vanish or explode gradient . He contribute to meta learning and propose flat minima as preferable solution of learn artificial neural network to ensure a low generalization error . He develop new activation function for neural network like exponential linear unit ( elus ) \\n  or scale elu ( selu ) to improve learn . He contribute to reinforcement learning via actor - critic approach \\n  and his rudder method . He apply biclustering method to drug discovery and toxicology . He extend support vector machine to handle kernel that be not positive definite with the \\\" potential support vector machine \\\" ( psvm ) model , and apply this model to feature selection , especially to gene selection for microarray datum . also in biotechnology , he develop \\\" factor analysis for robust microarray summarization \\\" ( farms ) . \", \"tokencount\": 220, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 227, \"title\": \"Sepp Hochreiter\", \"text\": \" in addition to his research contribution , sepp hochreiter be   broadly active within his field : he launch the bioinformatics working group at the austrian computer society ; he be found board member of different bioinformatic start - up company ; he be program chair of the conference bioinformatics research and development ; he be a conference chair of the conference critical assessment of massive data analysis ( camda ) ; and he be editor , program committee member , and review for international journal and conference . as a faculty member at johannes kepler linz , he found the bachelors program in bioinformatics , which be a cross - border , double - degree study program together with the university of south - bohemia in \\u010desk\\u00e9 bud\\u011bjovice ( budweis ) , czech republic . He also establish the masters program in bioinformatics , \\n where he be still the act dean of both study . \", \"tokencount\": 157, \"category\": \"Machine learning researchers\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 229, \"title\": \"Hyperparameter (machine learning)\", \"text\": \" in machine learning , a hyperparameter be a parameter whose value be set before the learning process begin . by contrast , the value of other parameter be derive via training . \", \"tokencount\": 32, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 229, \"title\": \"Hyperparameter (machine learning)\", \"text\": \" different model training algorithm require different hyperparameter , some simple algorithm ( such as ordinary least square regression ) require none . give these hyperparameter , the training algorithm learn the parameter from the datum . for instance , lasso be an algorithm that add a regularization hyperparameter to ordinary least square regression , which have to be set before estimate the parameter through the training algorithm . \", \"tokencount\": 68, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 230, \"title\": \"Epi Map\", \"text\": \" epi map be a module that display geographic map with datum from epi info . epi map be build around the esri mapobjects software . epi map display shapefil contain the geographic boundary layer with datum result from the analysis module . \", \"tokencount\": 42, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 230, \"title\": \"Epi Map\", \"text\": \" epi map be design to show datum from epi info 2000 file by relate datum field to shape file contain the geographic boundary . shapefile also can contain datum on population or other variable , and can therefore provide numeric datum that become part of the display either as numerator or denominator . dean , andrew g. et al . epi info 2000 manual \\u2013 epi info 2000 , a database , and statistics program for public health professionals . center for disease control and prevention ( cdc ) atlanta , georgia , usa \", \"tokencount\": 94, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 232, \"title\": \"InfiniteGraph\", \"text\": \" infinitegraph be an enterprise distribute graph database implement in java , and be from a class of nosql ( or not only sql ) database technology that focus on graph datum structure . developer use infinitegraph to find useful and often hide relationship in highly connected big datum set . \\n  infinitegraph be cross - platform , scalable , cloud enable , and be design to handle very high throughput . \", \"tokencount\": 71, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 232, \"title\": \"InfiniteGraph\", \"text\": \" infinitegraph be suit for application and service that solve graph problem or answer question such as \\\" how be I connect to kevin bacon ? \\\" or \\\" what be the cheap roundtrip flight from california to new york with no more than 2 stop , at least 30 minute between flight , and that depart at 8 a.m. tuesday and return by 6 p.m. friday ? \\\" \", \"tokencount\": 68, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 232, \"title\": \"InfiniteGraph\", \"text\": \" adoption be see in government , telco / networking , healthcare , cyber security , manufacturing , finance , crm , and social networking application . \", \"tokencount\": 26, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 240, \"title\": \"Stability (learning theory)\", \"text\": \" stability , also know as algorithmic stability , be a notion in computational learning theory of how a   machine learn algorithm be perturb by small change to its input . a stable learning algorithm be one for which the prediction do not change much when the training datum be modify slightly . for instance , consider a machine learn algorithm that be be train to recognize handwritten letter of the alphabet , use 1000 example of handwritten letter and their label ( \\\" a \\\" to \\\" z \\\" ) as a training set . one way to modify this training set be to leave out an example , so that only 999 example of handwritten letter and their label be available . a stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training set . \\n stability can be study for many type of learning problem , from language learn to inverse problem in physics and engineering , as it be a property of the learning process rather than the type of information be learn . the study of stability gain importance in computational learning theory in the 2000s when it be show to have a connection with generalization . It be show that for large class of learn algorithm , notably empirical risk minimization algorithm , certain type of stability ensure good generalization . \", \"tokencount\": 232, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 241, \"title\": \"Mlpy\", \"text\": \" mlpy be a python , open - source , machine learn library build on top of numpy / scipy , the gnu scientific library and it make an extensive use of the cython language . mlpy provide a wide range of state - of - the - art machine learn method for supervised and unsupervised problem and it be aim at find a reasonable compromise among modularity , maintainability , reproducibility , usability and efficiency . mlpy be multiplatform , it work with python 2 and 3 and it be distribute under gpl3 . \", \"tokencount\": 94, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 241, \"title\": \"Mlpy\", \"text\": \" suit for general - purpose machine learning task , soleymani et al ( 2011 ) . continuous emotion detection in response to music video . ieee international conference on automatic face & gesture recognition and workshops 2011.megies , t. et al ( 2011 ) . obspy \\u2013 what can it do for datum center and observatory ? annals of geophysics , 2011.nguyen , m. h ( 2010 ) . nguyen et al . optimal feature selection for support vector machine . pattern recognition , 2010.santana r. ( 2011 ) r. santana . estimation of distribution algorithm : from available implementation to potential development . proceeding of the 13th annual conference companion on genetic and evolutionary computation , 2011 . mlpy 's motivate application field be bioinformatic , i.e. the analysis of high throughput omic datum . wuchty s. ( 2010 ) . gene pathway and subnetwork distinguish between major glioma subtype and elucidate potential underlying biology . journal of biomedical informatics , 2010 \", \"tokencount\": 163, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 242, \"title\": \"Feature scaling\", \"text\": \" feature scaling be a method use to normalize the range of independent variable or feature of datum . in datum processing , it be also know as datum normalization and be generally perform during the datum preprocess step . \", \"tokencount\": 39, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 244, \"title\": \"MetaboAnalyst\", \"text\": \" metaboanalyst be a set of online tool for metabolomic datum analysis and interpretation , create by member of the wishart research group at the university of alberta . It be first release in may 2009 and version 2.0 be release in january 2012 . metaboanalyst provide a variety of analysis method that have be tailor for metabolomic datum . these method include metabolomic datum processing , normalization , multivariate statistical analysis , and datum annotation . the current version be focus on biomarker discovery and classification . \\n metaboanalyst support a wide variety of datum input type commonly generate by metabolomic study include gc / lc - ms raw spectra , ms / nmr peak list , nmr / ms peak intensity table , nmr / ms spectral bin , and metabolite concentration . \", \"tokencount\": 134, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 244, \"title\": \"MetaboAnalyst\", \"text\": \" metaboanalyst have four module : \\n  data processing \\n  statistical analysis ( one - factor , two - factor , and time - series datum ) \\n  functional enrichment analysis \\n  metabolic pathway analysis \", \"tokencount\": 34, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 244, \"title\": \"MetaboAnalyst\", \"text\": \" the table below summarize the main feature of each functional module . \\n  1 . data processing   2 . statistical analysis   3 . functional enrichment analysis peak detection   univariate analysis   over representation analysis   retention time correction   dimension reduction   single sample profiling   peak alignment   feature selection   quantitative enrichment analysis   baseline filtering   cluster analysis    data integrity check   classification   4 . metabolic pathway analysis missing value imputation   two - way anova   enrichment analysis    asca   topology analysis    temporal comparison   interactive visualization \", \"tokencount\": 98, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 244, \"title\": \"MetaboAnalyst\", \"text\": \" metaboanalyst generate a pdf report that include a write record of each analysis step and display result in graphical and tabular format . user can also download process data file and png image file . \", \"tokencount\": 35, \"category\": \"Data analysis software\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 244, \"title\": \"MetaboAnalyst\", \"text\": \" metaboanalyst be part of a suite of metabolomic database that also include human metabolome database ( hmdb ) , hmdb drugbank , drugbank toxin and toxin - target database , t3db and the small molecule pathway database . smpdb the hmdb have over 7900 human metabolite and roughly 7200 associated dna and protein sequence , that be link to these metabolite entry . while drugbank include information on 6707 drug and 4228 non - redundant drug target , enzyme , transporter , and carrier , t3db house over 2900 common toxin and environmental pollutant . the suite be round out by smpdb with its pathway diagram for more than 350 human metabolic and disease pathway . \", \"tokencount\": 116, \"category\": \"Data analysis software\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 246, \"title\": \"Heikki Mannila\", \"text\": \" heikki mannila ( bear 1960 ) be a finnish computer scientist , the president of the academy of finland . \\n mannila earn his ph.d. in 1985 from the university of helsinki under the supervision of esko ukkonenmathematical genealogy of heikki mannila from the international association for cryptologic research , retrieve 2012 - 02 - 25 . ( who be president of the academy of finland ) and for many year he be a professor at the university of helsinki himself . from 2004 to 2008 he be academy professor at the academy of finland . He become vice president for academic affairs at aalto university in 2009 , and be appoint by the finnish government as president of the academy of finland for a term last from 2012 to 2017.heikki mannila appoint as president of the academy of finland , finnish ministry of education and culture , december 19 , 2011 , retrieve 2012 - 02 - 25.heikki mannila take office as president of the academy of finland , university of helsinki computer science department , retrieve 2012 - 02 - 25 . the appointment be renew for the period 2017\\u20132022 . \", \"tokencount\": 193, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 246, \"title\": \"Heikki Mannila\", \"text\": \" mannila be know for his research in datum mining , and have publish highly cite paper on association rule learn . . . and sequence mining .. with david hand and padhraic smyth , he be the co - author of the book principles of data mining ( mit press , 2001).review of principles of data mining , ashish p sanil ( 2003 ) , j. amer . stat . assoc . 98 ( 461 ) : 252\\u2013253 , . \", \"tokencount\": 80, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 247, \"title\": \"LIONsolver\", \"text\": \" lionsolver be an integrated software for datum mining , business intelligence , analytic , and modeling \\n learning and intelligent optimization \\n  and reactive business intelligence approach . \\n  a non - profit version be available as lionoso . \", \"tokencount\": 39, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 247, \"title\": \"LIONsolver\", \"text\": \" lionsolver can be use to build model , visualize them , and improve business and engineering process . \\n It be a tool for decision make base on datum and quantitative model , it can be connect to most database \\n and external program , it be fully integrate with the grapheur business intelligence   software and intend for more advanced user , interested in design business logic and process and not only in simple analytic and visualization task . \", \"tokencount\": 80, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 249, \"title\": \"Trevor Hastie\", \"text\": \" trevor john hastie ( bear 27 june 1953 ) be a south african and american statistician and computer scientist . He be currently serve as the john a. overdeck professor of mathematical sciences and professor of statistics at the stanford university . hastie be know for his contribution to apply statistic , especially in the field of machine learning , datum mining , and bioinformatic . He have author several popular book in statistical learning , include the elements of statistical learning : data mining , inference , and prediction . hastie have be list as an isi highly cited author in mathematics by the isi web of knowledge . \", \"tokencount\": 110, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 250, \"title\": \"Representer theorem\", \"text\": \" in statistical learning theory , a representer theorem be any of several related result state that a minimizer f^ { * } of a regularize empirical risk functional define over a reproducing kernel hilbert space can be represent as a finite linear combination of kernel product evaluate on the input point in the training set datum . \", \"tokencount\": 57, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 251, \"title\": \"Symbolic data analysis\", \"text\": \" symbolic datum analysis ( sda ) be an extension of standard datum analysis where symbolic datum table be use as input and symbolic object be make output as a result . the datum unit be call symbolic since they be more complex than standard one , as they not only contain value or category , but also include internal variation and structure . sda be base on four space : the space of individual , the space of concept , the space of description , and the space of symbolic object . the space of description model individual , while the space of symbolic object model concept \\n . \", \"tokencount\": 109, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 253, \"title\": \"Maltego\", \"text\": \" maltego be proprietary software use for open - source intelligence and forensic , develop by paterva .   maltego focus on provide a library of transform for discovery of datum from open source , and visualize that information in a graph format , suitable for link analysis and datum mining . \", \"tokencount\": 51, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 253, \"title\": \"Maltego\", \"text\": \" maltego permit create custom entity , allow it to represent any type of information in addition to the basic entity type which be part of the software .   the basic focus of the application be analyze real - world relationship ( social network and computer network node ) between people , group , webpages , domain , network , internet infrastructure , and affiliation with online service such as twitter and facebook . among its datum source be dns record , whois record , search engine , online social network , various api and various meta datum . \", \"tokencount\": 99, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 255, \"title\": \"VIGRA\", \"text\": \" vigra be the abbreviation for \\\" vision with generic algorithms \\\" . It be a free open - source computer vision library which focus on customizable algorithm and datum structure . vigra component can be easily adapt to specific need of target application without compromise execution speed , by use template technique similar to those in the c++ standard template library . \", \"tokencount\": 62, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 257, \"title\": \"Google Flu Trends\", \"text\": \" google flu trends ( gft ) be a web service operate by google . It provide estimate of influenza activity for more than 25 country . by aggregate google search query , it attempt to make accurate prediction about flu activity . this project be first launch in 2008 by google.org to help predict outbreak of flu . \", \"tokencount\": 58, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 257, \"title\": \"Google Flu Trends\", \"text\": \" google flu trends be now no longer publish current estimate . historical estimate be still available for download , and current datum be offer for declare research purpose . \", \"tokencount\": 29, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 259, \"title\": \"Data analysis expressions\", \"text\": \" data analysis expressions ( dax ) be the native formula and query language for microsoft powerpivot , power bi desktop and sql server analysis services ( ssas ) tabular model . dax include some of the function that be use in excel formula with additional function that be design to work with relational datum and perform dynamic aggregation . It be , in part , an evolution of the multidimensional expression ( mdx ) language develop by microsoft for analysis services multidimensional model ( often call cube ) combine with excel formula function . It be design to be simple and easy to learn , while expose the power and flexibility of powerpivot and ssas tabular model . \", \"tokencount\": 118, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 260, \"title\": \"Universal portfolio algorithm\", \"text\": \" the universal portfolio algorithm be a portfolio selection algorithm from the field of machine learning and information theory . the algorithm learn adaptively from historical datum and maximize the log - optimal growth rate in the long run . It be introduce by the late stanford university information theorist thomas m. cover . \", \"tokencount\": 53, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 260, \"title\": \"Universal portfolio algorithm\", \"text\": \" the algorithm rebalanc the portfolio at the beginning of each trading period . at the beginning of the first trading period it start with a naive diversification . in the follow trading period the portfolio composition depend on the historical total return of all possible constant - rebalanc portfolio . \", \"tokencount\": 50, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 261, \"title\": \"Astrostatistics\", \"text\": \" astrostatistic be a discipline which span astrophysic , statistical analysis and datum mining . astrostatistic and astroinformatics portal It be use to process the vast amount of datum produce by automate scanning of the cosmo , to characterize complex dataset , and to link astronomical datum to astrophysical theory .   many branch of statistic be involve in astronomical analysis include nonparametric , multivariate regression and multivariate classification , time series analysis , and especially bayesian inference . \", \"tokencount\": 78, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 262, \"title\": \"DIVA software\", \"text\": \" divadiva homepage ( data - interpolate variational analysis ) allow the spatial interpolation / gridding of datum ( analysis ) in an optimal way , comparable to optimal interpolation ( oi ) , take into account uncertainty on observation . in comparison to standard oi , use in data assimilation , diva , when apply to ocean datum , take into account coastline , sub - basin and advection because of its variational formulation on the real domain . diva formulation calculation be highly optimize and rely on a finite element resolution . tool to generate the finite element mesh be provide as well as tool to optimize the parameter of the analysis . quality control of datum can be perform and error field can be calculate . troupin , c , barth , a , sirjacobs , d , ouberdous , m , brankart , j .- m , brasseur , p , rixen , m , alvera azcarate , a , belounis , m , capet , a , lenartz , f , toussaint , m .- e , & beckers , j .- m. ( 2012 ) . generation of analysis and consistent error field use the data interpolate variational analysis ( diva ) . ocean modelling , 52 - 53 , 90 - 101 . doi:10.1016/j.ocemod.2012.05.002 also detrend of datum be possible . finally 3d and 4d extension be include with emphasis on direct computation of climatology from odvodv homepage   spreadsheet file . \\n the software whose first version be available since 1996,brasseur , p. , beckers , j .- m. , brankart , j .- m. , and schoenauen , r. ( 1996a ) . seasonal temperature and salinity field in the mediterranean sea : climatological analysis of an historical data set . deep - sea research , 43(2):159 - 192 . can now be download at the diva site and be the reference tool for calculate climatology within the seadatanet project . It have also be include as the state - of - the art gridding method in ocean data view . \", \"tokencount\": 349, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 263, \"title\": \"Pipeline Pilot\", \"text\": \" pipeline pilot be a desktop software program develop by accelrys enterprise platform to process and analyze datum . originally use in the natural science , the product 's basic etl ( extract , transform , load ) and analytic capability have be broaden . the product be now use for data science , etl , reporting , prediction and analytic in a number of sector . the main feature of the product be the ability to design datum workflow use a graphical user interface . the program be an example of visual and dataflow programming . It have use in a variety of setting , such as   cheminformatic and qsar , next generation sequencing , image analysis , and text analytic . \", \"tokencount\": 123, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 265, \"title\": \"Oren Etzioni\", \"text\": \" oren etzioni ( b. 1964 ) be an american entrepreneur , professor of computer science , and ceo of the allen institute for artificial intelligence . He join the university of washington faculty in 1991 , where he become the washington research foundation entrepreneurship professor in the department of computer science and engineering . in may 2005 , he found and become the director of the university 's turing center . the center investigate problem in datum mining , natural language processing , the semantic web and other web search topic . etzioni coin the term machine reading and create the first commercial comparison shopping agent . \", \"tokencount\": 107, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 266, \"title\": \"Dlib\", \"text\": \" dlib be a general purpose cross - platform software library write in the programming language c++ . Its design be heavily influence by idea from design by contract and component - base software engineering . thus it be , first and foremost , a set of independent software component . It be open - source software release under a boost software license . \", \"tokencount\": 63, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 266, \"title\": \"Dlib\", \"text\": \" since development begin in 2002 , dlib have grow to include a wide variety of tool . as of 2016 , it contain software component for deal with networking , thread , graphical user interface , data structure , linear algebra , machine learning , image processing , datum mining , xml and text parse , numerical optimization , bayesian network , and many other task . in recent year , much of the development have be focus on create a broad set of statistical machine learning tool and in 2009 dlib be publish in the journal of machine learning research . since then it have be use in a wide range of domain . scholarly research use dlibdlib on mloss.orgautonome mobile systeme 2009ess : extremely simple serialization for c++yan , junchi , et al . \\\" online incremental regression for electricity price prediction . \\\" service operations and logistics , and informatics ( soli ) , 2012 ieee international conference on . ieee , 2012 . kuijf , hugo j. , max a. viergever , and koen l. vincken . \\\" automatic extraction of the curved midsagittal brain surface on mr images . \\\" medical computer vision . recognition techniques and applications in medical imaging . springer berlin heidelberg , 2013 . 225 - 232 . bormann , richard klaus eduard . \\\" vision - base place categorization . \\\" ( 2010).brodu , nicolas , and dimitri lague . \\\" 3d terrestrial lidar datum classification of complex natural scene use a multi - scale dimensionality criterion : application in geomorphology . \\\" isprs journal of photogrammetry and remote sensing 68 ( 2012 ) : 121\\u2013134.aung , zeyar , et al . \\\" towards accurate electricity load forecasting in smart grid . \\\" dbkda 2012 , the fourth international conference on advance in databases , knowledge , and data applications . 2012.rodriguez , alberto , et al . \\\" abort and retry in grasping . \\\" intelligent robots and systems ( iros ) , 2011 ieee / rsj international conference on . ieee , 2011 . mohan , vandana , et al . \\\" intraoperative prediction of tumor cell concentration from mass spectrometry imaging . \\\" int . symp . math . theo . netw . syst . 2010.nakashima , yuta , noboru babaguchi , and jianping fan . \\\" detect intend human object in human - capture video . \\\" computer vision and pattern recognition workshops ( cvprw ) , 2010 ieee computer society conference on . ieee , 2010 . \", \"tokencount\": 422, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 269, \"title\": \"Data-informed decision-making\", \"text\": \" data - inform decision - making ( didm ) give reference to the collection and analysis of datum to guide decision that improve success . u.s. department of education office of planning , evaluation and policy development ( 2009 ) . implement data - inform decision making in school : teacher access , support and use . united states department of education ( eric document reproduction service no . ed504191 ) didm be use in education community ( where datum be use with the goal of help student and improve curriculum ) but be also applicable to ( and thus also use in ) other field in which data be use to inform decision . while datum base decision making be a more common term , data - inform decision - making be a preferable term since decision should not be base solely on quantitative datum . knapp , m. s. , swinnerton , j. a. , copland , m. a. , & monpas - hubar , j. ( 2006 ) . data - inform leadership in education . seattle , wa : center for the study of teaching and policy . most educator have access to a data system for the purpose of analyze student datum . aaron , d. ( 2009 ) .    report find state on course to build pupil - data system . education week , 29(13 ) , 6 . these datum system present datum to educator in an over - the - counter datum format ( embed label , supplemental documentation , and a help system , make key package / display and content decision ) to improve the success of educators\\u2019 datum - inform decision - making . rankin , j. ( 2013 , march 28 ) . how datum systems & report can either fight or propagate the datum analysis error epidemic , and how educator leader can help . presentation conduct from technology information center for administrative leadership ( tical ) school leadership summit . in business , foster and actively support didm in their firm and among their colleague could be the main r\\u00f4le of cio ( chief information officers ) or cdos ( chief data officers).delort p. 2012 . iccp technology foresight forum - \\\" harnessing datum as a new source of growth : big datum analytic and policy \\\" . oecd , 2012 \", \"tokencount\": 396, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 8.182528469706376}, {\"docid\": 269, \"title\": \"Data-informed decision-making\", \"text\": \" assessment in high education be a form of didm aim at use evidence of what student learn to improve curriculum , student learning , and teaching . standardized test , grade , and student work score by rubric be form of student learn outcome assessment . there be numerous organization aim at promote the assessment of student learn through didm include the national institute for learning outcomes assessment , the association for the assessment of student learning in higher education , and , to an extent , the association of american colleges and universities . \", \"tokencount\": 95, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 272, \"title\": \"Tanagra (machine learning)\", \"text\": \" tanagra be a free suite of machine learn software for research and academic purpose \\n develop by ricco rakotomalala at the lumi\\u00e8re university lyon 2 , france . \\n tanagra support several standard datum mining task such as : visualization , descriptive statistic , instance selection , feature selection , feature construction , regression , factor analysis , clustering , classification and association rule learn . \", \"tokencount\": 66, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 272, \"title\": \"Tanagra (machine learning)\", \"text\": \" tanagra be an academic project . It be widely use in french - speak university . g. gregoire , f.x. jollois , j.f. petiot , a. qannari , s. sabourin , p. swertwaegher , j.c. turlot , v. vandewalle , s. viguier - pla , \\\" software and statistic teach in stid department of iut \\\" , in statistique et enseignement , 2(2 ) , 5 - 24 , 2011 . tanagra be frequently use in real studiess.g. jacob and r.g. ramani , \\\" evolve efficient clustering and classification patterns in lymphography data through data mining techniques \\\" , in international journal on soft computing ( ijsc ) , 3(3 ) , 119 - 132 , 2012.e. kirkos , c. spathis , a. nanopoulos , y. manolopoulos , \\\" identify qualified auditor 's opinion : a data mining approach \\\" , in journal of emerging technologies in accounting , 4(1 ) , 183 - 197 , 2007 . and in software comparison paper . r.m. rahman and f. afroz , \\\" comparison of various classification technique use different data mining tools for diabete diagnosis \\\" , in journal of software engineering and applications , 6 , 85 - 97 , 2013.h. solanki \\\" comparative study of data mining tools and analysis with unified data mining theory \\\" , in international journal of computer applications , 75(16 ) , 23 - 28 , 2013 . \", \"tokencount\": 234, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 273, \"title\": \"Programming with Big Data in R\", \"text\": \" programming with big data in r ( pbdr ) be a series of r package and an environment for statistical computing with big datum by use high - performance statistical computation . the pbdr use the same programming language as r with s3/s4 class and method which be use among statistician and datum miner for develop statistical software . the significant difference between pbdr and r code be that pbdr mainly focus on distribute memory system , where datum be distribute across several processor and analyze in a batch mode , while communication between processor be base on mpi that be easily use in large high - performance computing ( hpc ) system . r system mainly focus on single multi - core machine for datum analysis via an interactive mode such as gui interface . \", \"tokencount\": 136, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 273, \"title\": \"Programming with Big Data in R\", \"text\": \" two main implementation in r use mpi be rmpi and pbdmpi of pbdr. \\n  the pbdr build on pbdmpi use spmd parallelism where every processor be consider as worker and own part of datum . the spmd parallelism introduce in mid 1980 be particularly efficient in homogeneous computing environment for large datum , for example , perform singular value decomposition on a large matrix , or perform cluster analysis on high - dimensional large datum . on the other hand , there be no restriction to use manager / worker parallelism in spmd parallelism environment . \\n  the rmpi use manager / worker parallelism where one main processor ( manager ) server as the control of all other processor ( worker ) . the manager / worker parallelism introduce around early 2000 be particularly efficient for large task in small cluster , for example , bootstrap method and monte carlo simulation in applied statistic since i.i.d . assumption be commonly use in most statistical analysis . in particular , task pull parallelism have good performance for rmpi in heterogeneous computing environment . \\n the idea of spmd parallelism be to let every processor do the same amount of work , but on different part of a large datum set . for example , a modern gpu be a large collection of slow co - processor that can simply apply the same computation on different part of relatively small datum , but the spmd parallelism end up with an efficient way to obtain final solution ( i.e. time to solution be short ) . It be clear that pbdr be not only suitable for small cluster , but be also more stable for analyze big datum and more scalable for supercomputer . in short , pbdr \\n  do not like rmpi ,   nor parallel package in r , \\n  do not focus on interactive computing nor master / worker , \\n  but be able to use both spmd and task parallelism . \", \"tokencount\": 332, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 274, \"title\": \"H2O (software)\", \"text\": \" h2o be open - source software for big - data analysis . It be produce by the company h2o.ai . h2o allow user to fit thousand of potential model as part of discover pattern in datum . \", \"tokencount\": 37, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 274, \"title\": \"H2O (software)\", \"text\": \" the h2o software run can be call from the statistical package r , python , and other environment . It be use for explore and analyzing dataset hold in cloud computing system and in the apache hadoop distributed file system as well as in the conventional operating - system linux , macos , and microsoft windows . the h2o software be write in java , python , and r. Its graphical - user interface be compatible with four browser : chrome , safari , firefox , and internet explorer . \", \"tokencount\": 90, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 276, \"title\": \"IcCube\", \"text\": \" iccube be a company found in switzerland that provide business intelligence software of the same name . the solution can be fully embed as an integrate solution , can be host in a manage environment or instal locally , on premise . \", \"tokencount\": 42, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 276, \"title\": \"IcCube\", \"text\": \" the bi tool allow end - user to create or edit dashboard themselves and be capable of process datum from multiple source in real - time . the solution distinguish itself by make the dashboard , the dashboard builder , the schema / cube builder and the server monitoring application accessible from a browser only . no software have to be instal at the device of the end - user . \", \"tokencount\": 71, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 276, \"title\": \"IcCube\", \"text\": \" next to the browser - base dashboard builder , datum can be access by run query directly on the olap cube use mdx , sql or r. \", \"tokencount\": 27, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 278, \"title\": \"Apache Flume\", \"text\": \" apache flume be a distribute , reliable , and available software for efficiently collect , aggregate , and move large amount of log datum . It have a simple and flexible architecture base on stream datum flow . It be robust and fault tolerant with tunable reliability mechanism and many failover and recovery mechanism . It use a simple extensible datum model that allow for online analytic application . \", \"tokencount\": 69, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 279, \"title\": \"Wolfram Language\", \"text\": \" the wolfram language be a general multi - paradigm computational language develop by wolfram research and be the programming language of the mathematical symbolic computation program mathematica and the wolfram programming cloud . It emphasize symbolic computation , functional programming , and rule - base programming and can employ arbitrary structure and datum . \", \"tokencount\": 54, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 279, \"title\": \"Wolfram Language\", \"text\": \" It include build - in function for generate and run turing machine , create graphic and audio , analyzing 3d model , matrix manipulation , and solve differential equation . It be extensively document . \", \"tokencount\": 35, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 279, \"title\": \"Wolfram Language\", \"text\": \" wolfram language 's core principle that differentiate it from other programming language include a build - in knowledgebase , automation in the form of meta - algorithm and superfunction , a coherently elegant design and structure , build - in natural language understanding , and representation of everything as a symbolic expression.https://www.wolfram.com/language/principles/ \", \"tokencount\": 52, \"category\": \"Data mining and machine learning software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 279, \"title\": \"Wolfram Language\", \"text\": \" the wolfram language be release for the raspberry pi in 2013 with the goal of make it free for all raspberry pi user . It be include in the recommend software bundle that the raspberry pi foundation provide for beginner , which cause some controversy due to the wolfram language 's proprietary nature . plan to port the wolfram language to the intel edison be announce after the board 's introduction at ces 2014 . in 2019 , a link be add to make wolfram library compatible with the unity game engine , give game developer access to the language 's high level function . \", \"tokencount\": 105, \"category\": \"Data mining and machine learning software\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 280, \"title\": \"COMPLEAT (Bioinformatics tool)\", \"text\": \" protein complex enrichment analysis tool be an online bioinformatic tool use to analyze high - throughput dataset ( or small - scale dataset ) use protein complex enrichment analysis . the tool use a protein complex resource as the back end annotation datum instead of conventional gene ontology- or pathway - base annotation . the tool incorporate several useful feature in order to provide a comprehensive data - mining environment , include network - base visualization and interactive querying option . \\n compleat may be use to analyze rnai screen , proteomic dataset , gene expression datum and any other high - throughput dataset where protein complex information be relevant . \", \"tokencount\": 111, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 281, \"title\": \"Lise Getoor\", \"text\": \" lise getoor be a professor in the computer science department , at the university of california , santa cruz , and an adjunct professor in the computer science department at the university of maryland , college park . \\n Her primary research interest be in machine learning and reason with uncertainty , apply to graph and structured datum . \\n She also work in datum integration , social network analysis and visual analytic . She have multiple good paper award , an nsf career award , and be an association for the advancement of artificial intelligence ( aaai ) fellow . \\n She have edit a book on statistical relational learning that be a main reference in this domain . \\n She have publish many highly cite paper in academic journal and conference proceeding . \\n She have also serve as action editor for the machine learning journal , jair associate editor , and tkdd associate editor .  \\n She be a board member of the international machine learning society , have be a member of aaai executive council , be pc co - chair of icml 2011 , and have serve as senior pc member for conference include aaai , icml , ijcai , iswc , kdd , sigmod , uai , vldb , wsdm and www . \", \"tokencount\": 219, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 281, \"title\": \"Lise Getoor\", \"text\": \" She receive her ph.d. from stanford university , her m.s. from uc berkeley , and her b.s. from uc santa barbara . \\n prior to join university of california , santa cruz , she be a professor at the university of maryland , college park until nov 2013 . \", \"tokencount\": 49, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 282, \"title\": \"MagicPlot\", \"text\": \" magicplot be a technical plotting , curve fitting and datum analysis application . It provide a wide usage of the graphical user interface for datum exploration as well as various statistical analysis tool , peak fitting option , raster or vector format of publishable plot . \", \"tokencount\": 46, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 282, \"title\": \"MagicPlot\", \"text\": \" magicplot be a commercial software . the limited functional trial version be also available . \", \"tokencount\": 15, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 283, \"title\": \"Jerome H. Friedman\", \"text\": \" jerome harold friedman ( bear december 29 , 1939 ) be an american statistician , consultant and professor of statistics at stanford university , know for his contribution in the field of statistic and datum mining . jerome h. friedman professor of statistics . access 18 july 2017 . \", \"tokencount\": 49, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 284, \"title\": \"Bias\\u2013variance tradeoff\", \"text\": \" in statistic and machine learning , the bias \\u2013 variance tradeoff be the property of a set of predictive model whereby model with a low bias in parameter estimation have a high variance of the parameter estimate across sample , and vice versa . the bias \\u2013 variance dilemma or problem be the conflict in try to simultaneously minimize these two source of error that prevent supervise learn algorithm from generalize beyond their training set : \\n  the bias be an error from erroneous assumption in the learning algorithm . high bias can because an algorithm to miss the relevant relation between feature and target output ( underfitt ) . \\n  the variance be an error from sensitivity to small fluctuation in the training set . high variance can because an algorithm to model the random noise in the training datum , rather than the intend output ( overfitt ) . \", \"tokencount\": 151, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 284, \"title\": \"Bias\\u2013variance tradeoff\", \"text\": \" the bias \\u2013 variance decomposition be a way of analyze a learn algorithm 's expect generalization error with respect to a particular problem as a sum of three term , the bias , variance , and a quantity call the irreducible error , result from noise in the problem itself . \", \"tokencount\": 51, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 284, \"title\": \"Bias\\u2013variance tradeoff\", \"text\": \" this tradeoff apply to all form of supervised learning : classification , regression ( function fitting),bia \\u2013 variance decomposition , in encyclopedia of machine learning . eds . claude sammut , geoffrey i. webb . springer 2011 . pp . 100\\u2013101 and structured output learning . It have also be invoke to explain the effectiveness of heuristic in human learning . \", \"tokencount\": 61, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 285, \"title\": \"Kernel embedding of distributions\", \"text\": \" in machine learning , the kernel embedding of distribution ( also call the kernel mean or mean map ) comprise a class of nonparametric method in which a probability distribution be represent as an element of a reproducing kernel hilbert space   ( rkhs).a. smola , a. gretton , l. song , b. sch\\u00f6lkopf . ( 2007 ) . a hilbert space embedding for distributions . algorithmic learning theory : 18th international conference . springer : 13\\u201331 .    a generalization of the individual data - point feature mapping do in classical kernel method , the embedding of distribution into infinite - dimensional feature space can preserve all of the statistical feature of arbitrary distribution , while allow one to compare and manipulate distribution use hilbert space operation such as inner product , distance , projection , linear transformation , and spectral analysis . l. song , k. fukumizu , f. dinuzzo , a. gretton ( 2013 ) . kernel embeddings of conditional distributions : a unified kernel framework for nonparametric inference in graphical model . ieee signal processing magazine 30 : 98\\u2013111 .     this learn framework be very general and can be apply to distribution over any space \\\\omega   on which a sensible kernel function ( measure similarity between element of \\\\omega ) may be define .   for example , various kernel have be propose for learn from datum which be : vector in \\\\mathbb{r}^d , discrete class / category , string , graph / network , image , time series , manifold , dynamical system , and other structured object . j. shawe - taylor , n. christianini . ( 2004 ) . kernel methods for pattern analysis . cambridge university press , cambridge , uk.t. hofmann , b. sch\\u00f6lkopf , a. smola . ( 2008 ) . kernel methods in machine learning . the annals of statistics 36(3):1171\\u20131220 .   the theory behind kernel embedding of distribution have be primarily develop by   alex smola , le song , arthur gretton , and bernhard sch\\u00f6lkopf . a review of recent work on kernel embedding of distribution can be find in . \\n the analysis of distribution be fundamental in machine learning and statistic ,   and many algorithm in these field rely on information theoretic approach such as entropy , mutual information , or kullback \\u2013 leibler divergence .   however , to estimate these quantity , one must first either perform density estimation , or employ sophisticated space - partition / bias - correction strategy which be typically infeasible for high - dimensional datum . l. song . ( 2008 ) learning via hilbert space embedding of distributions . phd thesis , university of sydney .   commonly , method for model complex distribution rely on parametric assumption that may be unfounded or computationally challenging ( e.g. gaussian mixture model ) , while nonparametric method like kernel density estimation ( note : the smoothing kernel in this context have a different interpretation than the kernel discuss here ) or characteristic function representation ( via the fourier transform of the distribution ) break down in high - dimensional setting . \", \"tokencount\": 526, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 285, \"title\": \"Kernel embedding of distributions\", \"text\": \" method base on the kernel embedding of distribution sidestep these problem and also possess the follow advantage : \\n  data may be model without restrictive assumption about the form of the distribution and relationship between variable \\n   intermediate density estimation be not need \\n   practitioners may specify the property of a distribution most relevant for their problem ( incorporate prior knowledge via choice of the kernel ) \\n  if a characteristic kernel be use , then the embedding can uniquely preserve all information about a distribution , while thank to the kernel trick , computation on the potentially infinite - dimensional rkhs can be implement in practice as simple gram matrix operation \\n  dimensionality - independent rate of convergence for the empirical kernel mean ( estimate use sample from the distribution )   to the kernel embedding of the true underlie distribution can be prove . \\n  learn algorithm base on this framework exhibit good generalization ability and finite sample convergence , while often be simple and more effective than information theoretic method \\n thus , learn via the kernel embedding of distribution offer a principl drop - in replacement for information theoretic approach and be a framework which not only subsume many popular method in machine learning and statistic as special case , but also can lead to entirely new learn algorithm . \", \"tokencount\": 224, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 290, \"title\": \"Qloo\", \"text\": \" qloo ( pronounce \\\" clue \\\" ) be a company that use artificial intelligence ( ai ) to understand taste and cultural correlation . It provide company with an application programming interface ( api ) . It receive funding from leonardo dicaprio , elton john , barry sternlicht , pierre lagrange and other . \", \"tokencount\": 54, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 290, \"title\": \"Qloo\", \"text\": \" qloo establish consumer preference correlation via machine learn across datum span cultural domain include music , film , television , dining , nightlife , fashion , book and travel . the recommender system use ai to predict correlation for further application . \", \"tokencount\": 42, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 294, \"title\": \"DADiSP\", \"text\": \" dadisp ( data analysis and display , pronounced day - disp ) be a numerical computing environment develop by dsp development corporation which allow one to display and manipulate datum series , matrix and image with an interface similar to a spreadsheet . dadisp be use in the study of signal processing , numerical analysis , statistical and physiological datum processing . \", \"tokencount\": 62, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 296, \"title\": \"Apache Spark\", \"text\": \" apache spark be an open - source distribute general - purpose cluster - compute framework . spark provide an interface for program entire cluster with implicit datum parallelism and fault tolerance . originally develop at the university of california , berkeley 's amplab , the spark codebase be later donate to the apache software foundation , which have maintain it since . \", \"tokencount\": 62, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 297, \"title\": \"Kernel methods for vector output\", \"text\": \" kernel method be a well - establish tool to analyze the relationship between input datum and the correspond output of a function . kernel encapsulate the property of function in a computationally efficient way and allow algorithm to easily swap function of vary complexity . \\n in typical machine learn algorithm , these function produce a scalar output . recent development of kernel method for function with vector - value output be due , at least in part , to interest in simultaneously solve relate problem . kernel which capture the relationship between the problem allow them to borrow strength from each other . algorithms of this type include multi - task learning ( also call multi - output learning or vector - value learning ) , transfer learning , and co - kriging . multi - label classification can be interpret as mapping input to ( binary ) cod vector with length equal to the number of class . \", \"tokencount\": 160, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 297, \"title\": \"Kernel methods for vector output\", \"text\": \" in gaussian process , kernel be call covariance function . multiple - output function correspond to consider multiple process . see bayesian interpretation of regularization for the connection between the two perspective . \", \"tokencount\": 33, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 298, \"title\": \"SensoMotoric Instruments\", \"text\": \" sensomotoric instrument ( smi)smi company . smi . retrieve march 13 , 2017 . be a german provider of dedicated computer vision application with a major focus on eye tracking technology . smi be found in 1991 as a spin - off from academic and medical research at the free university of berlin . the company have its headquarters in teltow near berlin , germany , office in boston , massachusetts and san francisco , california , in the united states , and a worldwide distributor and partner network . \", \"tokencount\": 90, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 298, \"title\": \"SensoMotoric Instruments\", \"text\": \" smi provide eye tracking system for scientific research , professional solution and oem application . the eye tracker can be combine with motion tracking system , ben coxworth ( may 9 , 2014 ) two - part system track body movement and gaze . gizmag . retrieve april 17 , 2015 . eeg , emotiv and smi combine remote eye tracking and eeg : a perfect match ! . reuters . retrieve april 3 , 2014.hisham abboud ( january 4 , 2014 )   new : cedrus introduces stimtracker for smi eye trackers . cedrus . retrieve april 3 , 2014 . and other biometric datum . sensomotoric instruments and noldus information technology combine eye tracking and video analysis . noldus . retrieve april 2 , 2014 . They can be integrate into virtual reality cave , ben lang ( february 8 , 2013 ) smi introduces 3d glass with eye tracking . road to vr . retrieve april 3 , 2014 . head - mount display - such as google glasssmi gaze interaction powers google glass prototype . reuters . retrieve april 17 , 2015.stephen hall ( april 16 , 2015 ) . smi \\u2019s google glass eye - tracking prototype be impressive . google glass fans . retrieve april 17 , 2015 . or oculus rift , jamie feltham ( november 17 , 2014 ) . smi launches oculus rift eye tracking upgrade package . vr focus . retrieve april 17 , 2015.dave leclair ( november 21 , 2014 ) . smi announce eye - tracking upgrade for oculus rift . gizmag . retrieve april 17 , 2015.oliver kreylos ( june 2 , 2014 ) . an eye - track oculus rift . doc - ok . retrieve april 17 , 2015 . simulator , car , or computer as a measurement or interaction modality . \", \"tokencount\": 308, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 299, \"title\": \"Piranha (software)\", \"text\": \" piranha be a text mining system develop for the united states department of energy ( doe ) by oak ridge national laboratory ( ornl ) .   the software process large volume of unrelated free - text document and show relationship amongst them , a technique valuable across numerous scientific and datum domain , from health care fraud to national security .   the result be present in cluster of prioritized relevance to business and government analyst . piranha use the term frequency / inverse corpus frequency term weighting method which provide strong parallel processing of textual information , thus the ability to analyze very large document set . \\n piranha have six main strength : \\n collecting and extracting : million of document from numerous source such as database and social medium can be collect and text extract from hundred of file format ; this info . can then be translate to any number of language . \\n store and indexing : document in search server , relational database , etc . can be store and index at will . \\n recommending : recommend the most valuable information for particular user . \\n categorizing : group item via supervised and semi - supervised machine learning method and target search list . \\n clustering : similarity be use to create a hierarchical group of document . \\n visualizing : show relationship among document so that user can quickly recognize connection . \", \"tokencount\": 241, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 299, \"title\": \"Piranha (software)\", \"text\": \" this work have result in eight issue ( 9,256,649 , 8,825,710 , 8,473,314 , 7,937,389 , 7,805,446 , 7,693,9037 , 7,315,858 , 7,072,883 ) , and several commercial license ( include textore and pro2serve ) , a spin - off company with the inventor , covenant health , and pro2serve call vortext analytics , two r&d 100 awards , and score of peer review research publication . \", \"tokencount\": 67, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 302, \"title\": \"Inductive probability\", \"text\": \" inductive probability attempt to give the probability of future event base on past event . It be the basis for inductive reasoning , and give the mathematical basis for learning and the perception of pattern . It be a source of knowledge about the world . \\n there be three source of knowledge : inference , communication , and deduction . communication relay information find use other method .   deduction establish new fact base on exist fact .   only inference establish new fact from datum . \", \"tokencount\": 88, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 302, \"title\": \"Inductive probability\", \"text\": \" the basis of inference be bayes ' theorem . but this theorem be sometimes hard to apply and understand . the simple method to understand inference be in term of quantity of information . \", \"tokencount\": 34, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 302, \"title\": \"Inductive probability\", \"text\": \" information describe the world be write in a language . for example , a simple mathematical language of proposition may be choose . sentence may be write down in this language as string of character .   but in the computer it be possible to encode these sentence as string of bit ( 1 and 0s ) . then the language may be encode so that the most commonly use sentence be the short . this internal language implicitly represent probability of statement . \", \"tokencount\": 84, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 302, \"title\": \"Inductive probability\", \"text\": \" occam 's razor say the \\\" simple theory , consistent with the datum be most likely to be   correct \\\" . the \\\" simple theory \\\" be interpret as the representation of the theory write in this internal language . the theory with the short encoding in this internal language be most likely to be correct . \", \"tokencount\": 58, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 303, \"title\": \"Sisense\", \"text\": \" sisense be a business analytic software company with office in new york city ,   san francisco , tel aviv , london , melbourne , tokyo , and scottsdale , arizona . Its business intelligence product include both a back - end power by in - chip technology that enable non - technical user to join and analyze large datum set from multiple source , and a front - end for create visualization , like dashboard and report , on any device , include mobile . \", \"tokencount\": 86, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 305, \"title\": \"WINEPI\", \"text\": \" in datum mining , the winepi algorithm be an influential algorithm for episode mining , which help discover the knowledge hide in an event sequence . \\n winepi derive part of its name from the fact that it use a slide window to go through the event sequence . \", \"tokencount\": 49, \"category\": \"Data mining algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 305, \"title\": \"WINEPI\", \"text\": \" the outcome of the algorithm be episode rule describe temporal relationship between event and form an extension of association rule . \", \"tokencount\": 21, \"category\": \"Data mining algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 306, \"title\": \"Yooreeka\", \"text\": \" yooreeka be a library for datum mining , machine learning , soft computing , and mathematical analysis . the project start with the code of the book \\\" algorithms of the intelligent web \\\" . although the term \\\" web \\\" prevail in the title , in essence , the algorithm be valuable in any software application . \", \"tokencount\": 58, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 306, \"title\": \"Yooreeka\", \"text\": \" yooreeka 2.x be license under the apache license rather than the somewhat more restrictive lgpl ( which be the license of v1.x ) . \", \"tokencount\": 24, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 306, \"title\": \"Yooreeka\", \"text\": \" the library be write 100 % in the java language . \", \"tokencount\": 11, \"category\": \"Data mining and machine learning software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" data exploration be an approach similar to initial datum analysis , whereby a data analyst use visual exploration to understand what be in a dataset and the characteristic of the datum , rather than through traditional data management systemsfoster open science , overview of data exploration techniques : stratos idreos , olga papaemmonouil , surajit chaudhuri .. these characteristic can include size or amount of datum , completeness of the datum , correctness of the datum , possible relationship amongst datum element or file / table in the datum . \\n data exploration be typically conduct use a combination of automated and manual activity . stanford.edu , 2011 wrangler : interactive visual specification of data transformation scripts , kandel , paepcke , hellerstein heer . automate activity can include datum profiling or datum visualization or tabular report to give the analyst an initial view into the datum and an understanding of key characteristic . foster open science , overview of data exploration techniques : stratos idreos , olga papaemmonouil , surajit chaudhuri . \", \"tokencount\": 173, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 6.188022575493998}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" this be often follow by manual drill - down or filtering of the datum to identify anomaly or pattern identify through the automate action .   data exploration can also require manual scripting and query into the datum ( e.g. use language such as sql or r ) or use excel or similar tool to view the raw data . stanford.edu , ieee visual analytics science & technology ( vast ) , oct 2012 enterprise data analysis and visualization : an interview study . , sean kandel , andreas paepcke , joseph hellerstein , jeffrey heer proc . \", \"tokencount\": 98, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" all of these activity be aim at create a clear mental model and understanding of the datum in the mind of the analyst , and define basic metadata ( statistic , structure , relationship ) for the datum set that can be use in further analysis . foster open science , overview of data exploration techniques : stratos idreos , olga papaemmonouil , surajit chaudhuri . \", \"tokencount\": 66, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" once this initial understanding of the data be have , the datum can be prune or refine by remove unusable part of the datum , correct poorly format element and define relevant relationship across dataset . this process be also know as determine datum quality . \", \"tokencount\": 46, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" datum exploration can also refer to the ad hoc querying and visualization of datum to identify potential relationship or insight that may be hide in the datum . \", \"tokencount\": 28, \"category\": \"Machine learning\", \"lineid\": 4, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 307, \"title\": \"Data exploration\", \"text\": \" traditionally , this have be a key area of focus for statistician , with john tukey be a key evangelist in the field .   exploratory data analysis , pearson . . today , data exploration be more widespread and be the focus of datum analyst and datum scientist ; the latter be a relatively new role within enterprise and large organization . \", \"tokencount\": 63, \"category\": \"Machine learning\", \"lineid\": 5, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 313, \"title\": \"Quantum machine learning\", \"text\": \" quantum machine learning be an emerge interdisciplinary research area at the intersection of quantum physics and machine learning .   the most common use of the term refer to machine learn algorithm for the analysis of classical datum execute on a quantum computer . while machine learn algorithm be use to compute immense quantity of datum , quantum machine learning increase such capability intelligently , by create opportunity to conduct analysis on quantum state and system . this include hybrid method that involve both classical and quantum processing , where computationally difficult subroutine be outsource to a quantum device . these routine can be more complex in nature and execute faster with the assistance of quantum device . furthermore , quantum algorithm can be use to analyze quantum state instead of classical datum . beyond quantum computing , the term \\\" quantum machine learning \\\" be often associate with machine learn method apply to datum generate from quantum experiment , such as learn quantum phase transition or create new quantum experiment . quantum machine learn also extend to a branch of research that explore methodological and structural similarity between certain physical system and learning system , in particular neural network . for example , some mathematical and numerical technique from quantum physics be applicable to classical deep learning and vice versa . finally , researcher investigate more abstract notion of learn theory with respect to quantum information , sometimes refer to as \\\" quantum learn theory \\\" . \", \"tokencount\": 248, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 314, \"title\": \"Health care analytics\", \"text\": \" health care analytic be the healthcare analysis activity that can be undertake as a result of datum collect from four area within healthcare ; claim and cost datum , pharmaceutical and research and development ( r&d ) datum , clinical datum ( collect from electronic medical record ( ehrs ) ) , and patient behavior and sentiment datum ( patient behavior and preference , ( retail purchase e.g. datum capture in run store ) . health care analytic be a grow industry in the united states , expect to grow to more than $ 31 billion by 2022 . the industry focus on the area of clinical analysis , financial analysis , supply chain analysis , as well as , fraud and hr analysis . \", \"tokencount\": 125, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 314, \"title\": \"Health care analytics\", \"text\": \" health care analytic allow for the examination of pattern in various healthcare datum in order to determine how clinical care can be improve while limit excessive spending . \", \"tokencount\": 28, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 317, \"title\": \"Social media mining\", \"text\": \" social medium mining be the process of obtain big datum from user - generate content on social medium site and mobile app in order to extract pattern , form conclusion about user , and act upon the information , often for the purpose of advertising to user or conduct research . the term be an analogy to the resource extraction process of mining for rare mineral . resource extraction mining require mining company to sift through vast quantity of raw ore to find the precious mineral ; likewise , social medium mining require human datum analyst and automate software program to sift through massive amount of raw social medium datum in order to discern pattern and trend relate to social medium usage , online behaviour , sharing of content , connection between individual , online buying behaviour , and more . these pattern and trend be of interest to company , government and not - for - profit organization , as these organization can use these pattern and trend to design their strategy or introduce new program , new product , process or service . \", \"tokencount\": 185, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 317, \"title\": \"Social media mining\", \"text\": \" social medium mining use a range of basic concept from computer science , datum mining , machine learning and statistic . social medium miner develop algorithm suitable for investigate massive file of social medium datum . social medium mining be base on theory and methodology from   social network analysis , network science , sociology , ethnography , optimization and mathematic . It encompass the tool to formally represent , measure and model meaningful pattern from large - scale social medium datum . in the 2010s , major corporation , government and not - for - profit organization engage in social medium mining to obtain datum about customer , client and citizen . \", \"tokencount\": 113, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 320, \"title\": \"Multiple kernel learning\", \"text\": \" multiple kernel learning refer to a set of machine learning method that use a predefined set of kernel and learn an optimal linear or non - linear combination of kernel as part of the algorithm . reason to use multiple kernel learning include a ) the ability to select for an optimal kernel and parameter from a large set of kernel , reduce bias due to kernel selection while allow for more automate machine learning method , and b ) combine datum from different source ( e.g. sound and image from a video ) that have different notion of similarity and thus require different kernel . instead of create a new kernel , multiple kernel algorithm can be use to combine kernel already establish for each individual datum source . \", \"tokencount\": 130, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 320, \"title\": \"Multiple kernel learning\", \"text\": \" multiple kernel learning approach have be use in many application , such as event recognition in video , lin chen , lixin duan , and dong xu , \\\" event recognition in videos by learn from heterogeneous web sources , \\\" in ieee international conference on computer vision and pattern recognition ( cvpr ) , 2013 , pp . 2666 - 2673 object recognition in image , serhat s. bucak , rong jin , and anil k. jain , multiple kernel learn for visual object recognition : a review . t - pami , 2013 . and biomedical datum fusion . yu et al . l2-norm multiple kernel learning and its application to biomedical datum fusion . bmc bioinformatic 2010 , 11:309 \", \"tokencount\": 122, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 324, \"title\": \"Archetypal analysis\", \"text\": \" archetypal analysis in the statistic be an unsupervised learning method similar to the cluster analysis and introduce by adele cutler and leo breiman in 1994 . rather than \\\" typical \\\" observation ( cluster center ) , it seek extremal point in the multidimensional datum , the \\\" archetype \\\" . the archetype be convex combination of observation choose so that observation can be approximate by convex combination of the archetype . \", \"tokencount\": 72, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 325, \"title\": \"Adversarial machine learning\", \"text\": \" adversarial machine learning be a technique employ in the field of machine learning which attempt to fool model through malicious input . this technique can be apply for a variety of reason , the most common being to attack or because a malfunction in standard machine learning model . \", \"tokencount\": 49, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 325, \"title\": \"Adversarial machine learning\", \"text\": \" machine learning technique be originally design for stationary and benign environment in which the training and test datum be assume to be generate from the same statistical distribution . however , when those model be implement in the real world , the presence of intelligent and adaptive adversary may violate that statistical assumption to some degree , depend on the adversary . this technique show how a malicious adversary can surreptitiously manipulate the input datum so as to exploit specific vulnerability of learn algorithm and compromise the security of the machine learn system . \", \"tokencount\": 94, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 328, \"title\": \"Distributed R\", \"text\": \" distribute r be an open source , high - performance platform for the r language . It split task between multiple processing node to reduce execution time and analyze large datum set . distribute r enhance r by add distribute datum structure , parallelism primitiv to run function on distribute datum , a task scheduler , and multiple datum loader . It be mostly use to implement distribute version of machine learning task . distribute r be write in c++ and r , and retain the familiar look and feel of r. , hewlett - packard ( hp ) provide enterprise support for distributed r with proprietary addition such as a fast datum loader from the vertica database . \", \"tokencount\": 119, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 332, \"title\": \"Cubes (OLAP server)\", \"text\": \" cubes be a light - weight open source multidimensional modelling and olap toolkit for development reporting application and browse of aggregated datum write in python programming language release under the mit license . \", \"tokencount\": 33, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 332, \"title\": \"Cubes (OLAP server)\", \"text\": \" cube provide to an analyst or any application end - user \\\" understandable and natural way of reporting use concept of datum cubes \\u2013 multidimensional datum object \\\" . \", \"tokencount\": 29, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 332, \"title\": \"Cubes (OLAP server)\", \"text\": \" cube be first publicly release in march 2011 . the project be originally develop for public procurements of slovakia . public procurements of slovakia by transparency international slovakia cubes 1.0 be release in september 2014 and present on the pydata conference in new yorkcubes 1.0 overview at pydata nyc 2014 ( video ) . \", \"tokencount\": 54, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 335, \"title\": \"Neural Designer\", \"text\": \" neural designer be a software tool for data analytic base on neural network , a main area of artificial intelligence research , and contain a graphical user interface which simplify datum entry and interpretation of result . \", \"tokencount\": 37, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 335, \"title\": \"Neural Designer\", \"text\": \" in 2015 , neural designer be choose by the european commission , within the horizon 2020 program , as a disruptive technology in the ict field . \", \"tokencount\": 27, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 338, \"title\": \"User behavior analytics\", \"text\": \" user behavior analytic ( uba ) as define by gartner be a cybersecurity process about detection of insider threat , target attack , and financial fraud . uba solution look at pattern of human behavior , and then apply algorithm and statistical analysis to detect meaningful anomaly from those pattern \\u2014 anomaly that indicate potential threat . market guide for user behavior analytics   instead of track device or security event , uba track a system 's user . the hunt for data analytic : be your siem on the endanger list ? big datum platform like apache hadoop be increase uba functionality by allow them to analyze petabyte worth of datum to detect insider threat and advanced persistent threat . \", \"tokencount\": 121, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 339, \"title\": \"Error level analysis\", \"text\": \" error level analysis ( ela ) be the analysis of compression artifact in digital datum with lossy compression such as jpeg . \", \"tokencount\": 22, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 341, \"title\": \"BisQue (Bioimage Analysis and Management Platform)\", \"text\": \" bisque be a free , open source web - base platform for the exchange and exploration of large , complex dataset . It be be develop at the vision research lab   vision research lab homepage at the university of california , santa barbara . bisque specifically support large scale , multi - dimensional multimodal - image and image analysis . metadata be store as arbitrarily nest and link tag / value pair , allow for domain - specific datum organization . image analysis module can be add to perform complex analysis task on compute cluster . analysis result be store within the database for further querying and processing . the datum and analysis provenance be maintain for reproducibility of result . bisque can be easily deploy in cloud compute environment or on computer cluster for scalability . bisque have be integrate into the nsf cyberinfrastructure project cyverse . cyverse cyberinfrastructure homepage the user interact with bisque via any modern web browser . \", \"tokencount\": 163, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 342, \"title\": \"Fluentd\", \"text\": \" fluentd be a cross platform open - source datum collection software project originally develop at treasure data . It be write primarily in the ruby programming language . \", \"tokencount\": 28, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 343, \"title\": \"Poimapper\", \"text\": \" poimapper be field datum collection , sharing and analyse software . \", \"tokencount\": 11, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 343, \"title\": \"Poimapper\", \"text\": \" mobile application be use to collect datum and update datum . by upload datum to a cloud server it be share among other mobile and office worker . \", \"tokencount\": 28, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 343, \"title\": \"Poimapper\", \"text\": \" poimapper be develop by pajat solutions ltd. pajat solutions be found in 2009 and be headquarter in finland . It create mobile solution for field reporting.https://www.crunchbase.com/organization/pajat-solution . in 2013 pajat be award the european csr award for innovative , non - business partnership that have help to solve social problem while create business advantage . the award come from a partnership where ngo 's like plan international be use poimapper in their health - relate monitoring and evaluation work . \", \"tokencount\": 80, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 346, \"title\": \"Dark data\", \"text\": \" dark datum be datum which be acquire through various computer network operation but not use in any manner to derive insight or for decision making . the ability of an organisation to collect datum can exceed the throughput at which it can analyse the datum . in some case the organisation may not even be aware that the data be be collect . ibm estimate that roughly 90 percent of datum generate by sensor and analog - to - digital conversion never get use . \\n in an industrial context , dark datum can include information gather by sensor and telematic . \", \"tokencount\": 102, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 346, \"title\": \"Dark data\", \"text\": \" organization retain dark datum for a multitude of reason , and it be estimate that most company be only analyze 1 % of their datum . the big datum challenge of transformation for the manufacturing industry often it be store for regulatory complianceare you use your dark datum effectively and record keeping . some organization believe that dark datum could be useful to them in the future , once they have acquire good analytic and business intelligence technology to process the information . because storage be inexpensive , store datum be easy . however , store and secure the datum usually entail great expense ( or even risk ) than the potential return profit . \", \"tokencount\": 115, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 4.19351668128162}, {\"docid\": 347, \"title\": \"Stochastic block model\", \"text\": \" the stochastic block model be a generative model for random graph . this model tend to produce graph contain community , subset characterize by be connect with one another with particular edge density . for example , edge may be more common within community than between community . the stochastic block model be important in statistic , machine learning , and network science , where it serve as a useful benchmark for the task of recover community structure in graph datum . \", \"tokencount\": 82, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 352, \"title\": \"Out-of-bag error\", \"text\": \" out - of - bag ( oob ) error , also call out - of - bag estimate , be a method of measure the prediction error of random forest , boost decision tree , and other machine learning model utilize bootstrap aggregating ( bag ) to sub - sample datum sample use for training . oob be the mean prediction error on each training sample , use only the tree that do not have   in their bootstrap sample . \", \"tokencount\": 81, \"category\": \"Machine learning algorithms\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 352, \"title\": \"Out-of-bag error\", \"text\": \" subsampling allow one to define an out - of - bag estimate of the prediction performance improvement by evaluate prediction on those observation which be not use in the building of the next base learner . out - of - bag estimate help avoid the need for an independent validation dataset , but often underestimate actual performance improvement and the optimal number of iteration . \", \"tokencount\": 65, \"category\": \"Machine learning algorithms\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 353, \"title\": \"Sparse dictionary learning\", \"text\": \" sparse dictionary learning be a representation learning method which aim at find a sparse representation of the input datum ( also know as sparse coding ) in the form of a linear combination of basic element as well as those basic element themselves . these element be call atom and they compose a dictionary . atom in the dictionary be not require to be orthogonal , and they may be an over - complete spanning set . this problem setup also allow the dimensionality of the signal be represent to be high than the one of the signal be observe . the above two property lead to have seemingly redundant atom that allow multiple representation of the same signal but also provide an improvement in sparsity and flexibility of the representation . \", \"tokencount\": 132, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 353, \"title\": \"Sparse dictionary learning\", \"text\": \" one of the most important application of sparse dictionary learning be in the field of compressed sensing or signal recovery . in compressed sensing , a high - dimensional signal can be recover with only a few linear measurement provide that the signal be sparse or nearly sparse . since not all signal satisfy this sparsity condition , it be of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterize matrix . once a matrix or a high dimensional vector be transfer to a sparse space , different recovery algorithm like basis pursuit , cosamp or fast non - iterative algorithmslotfi , m. ; vidyasagar , m.\\\"a fast non - iterative algorithm for compressive sensing using binary measurement matrices \\\" can be use to recover the signal . \", \"tokencount\": 141, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 353, \"title\": \"Sparse dictionary learning\", \"text\": \" one of the key principle of dictionary learning be that the dictionary have to be infer from the input datum . the emergence of sparse dictionary learning method be stimulate by the fact that in signal process one typically want to represent the input datum use as few component as possible . before this approach the general practice be to use predefined dictionary ( such as fourier or wavelet transform ) . however , in certain case a dictionary that be train to fit the input datum can significantly improve the sparsity , which have application in datum decomposition , compression and analysis and have be use in the field of image denoising and classification , video and audio processing . sparsity and overcomplete dictionary have immense application in image compression , image fusion and inpaint .   \", \"tokencount\": 138, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 357, \"title\": \"List of datasets for machine-learning research\", \"text\": \" these dataset be use for machine - learn research and have be cite in peer - review academic journal . dataset be an integral part of the field of machine learning . major advance in this field can result from advance in learn algorithm ( such as deep learning ) , computer hardware , and , less - intuitively , the availability of high - quality training dataset . high - quality label training dataset for supervised and semi - supervised machine learn algorithm be usually difficult and expensive to produce because of the large amount of time need to label the datum . although they do not need to be label , high - quality dataset for unsupervised learning can also be difficult and costly to produce . weiss , gary m. , and foster provost . \\\" learn when training datum be costly : the effect of class distribution on tree induction . \\\" journal of artificial intelligence research ( 2003 ) : 315\\u2013354.turney , peter . \\\" type of cost in inductive concept learn . \\\" ( 2000).abney , steven . semisupervised learn for computational linguistic . crc press , 2007.\\u017eliobait\\u0117 , indr\\u0117 , et al . \\\" active learn with evolve streaming datum . \\\" machine learning and knowledge discovery in databases . springer berlin heidelberg , 2011 . 597\\u2013612 . \", \"tokencount\": 225, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 361, \"title\": \"Chih-Jen Lin\", \"text\": \" chih - jen lin ( ) be distinguished professor of computer science at national taiwan university , and a leading researcher in machine learning , optimization , and datum mining . He be best know for the open source library libsvm , an implementation of support vector machine . \", \"tokencount\": 49, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 363, \"title\": \"Apache SystemML\", \"text\": \" apache systemml be a flexible machine learn system that automatically scale to spark and hadoop cluster . systemml 's \\n distinguishing characteristic be : \", \"tokencount\": 24, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 363, \"title\": \"Apache SystemML\", \"text\": \"   algorithm customizability via r - like and python - like language . \\n  multiple execution mode , include standalone , spark batch , spark mlcontext , hadoop batch , and jmlc . \\n  automatic optimization base on datum and cluster characteristic to ensure both efficiency and scalability . \", \"tokencount\": 49, \"category\": \"Data mining and machine learning software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 371, \"title\": \"Bayesian structural time series\", \"text\": \" bayesian structural time series ( bsts ) model be a machine learning technique use for feature selection , time series forecasting , nowcasting , infer causal impact and other application . the model be design to work with time series datum . \", \"tokencount\": 42, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 371, \"title\": \"Bayesian structural time series\", \"text\": \" the model have also promising application in the field of analytical marketing . in particular , it can be use in order to assess how much different marketing campaign have contribute to the change in web search volume , product sale , brand popularity and other relevant indicator . difference - in - difference model and interrupted time series design be alternative to this approach . \\\" in contrast to classical difference - in - difference scheme , state - space model make it possible to ( i ) infer the temporal evolution of attributable impact , ( ii ) incorporate empirical prior on the parameter in a fully bayesian treatment , and ( iii ) flexibly accommodate multiple source of variation , include the time - vary influence of contemporaneous covariate , i.e. , synthetic control . \\\" \", \"tokencount\": 139, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 374, \"title\": \"UNISoN (Social Network Analysis Tool)\", \"text\": \" unison be a java application that can download usenet message from free nntp server , show the save message , then allow filtering of datum to save to a pajek network file or csv file . It create network use the author of each post . if someone reply to a post , there be a unidirectional link create from the author of the post to the author of the message they be reply to . there be also a preview panel that show the network visually . It be develop in 2008 as part of an msc business systems analysis & design at city university london . project website(unison.sleonard.co.uk ) and be release as freeware . in 2016 the code be make open source . github / unison/(github ) \", \"tokencount\": 130, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 375, \"title\": \"Continuous analytics\", \"text\": \" continuous analytic be a data science process that abandon etl and complex batch datum pipeline in favor of cloud - native and microservice paradigms . continuous datum processing enable realtime interaction and immediate insight with few resource . \", \"tokencount\": 38, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 376, \"title\": \"Bing Predicts\", \"text\": \" bing predict be a prediction engine develop by microsoft that use machine learn from datum on trend social medium topic ( and sentiment towards those topic ) , along with trend search on bing . It predict the outcome of political election , popular reality show , and major sporting event . prediction can be access through the bing search engine . \", \"tokencount\": 62, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 380, \"title\": \"Dataiku\", \"text\": \" dataiku be a computer software company headquarter in new york city . the company develop collaborative datum science software market for big datum . \", \"tokencount\": 24, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 381, \"title\": \"Open coding\", \"text\": \" open coding in ground theory method be the analytic process by which concept ( code ) to the observed datum and phenomenon be attach during qualitative datum analysis . It be one of the ' procedure ' for work with text as characterize by strauss ( 1987 ) and strauss and corbin ( 1990 ) . open coding aim at develop substantial code describe , name or classify the phenomenon under consideration . open coding be achieve by segment datum into meaningful expression and describe them in single word or short sequence of word . further , relevant annotation and concept be then attach to these expression . \", \"tokencount\": 108, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 381, \"title\": \"Open coding\", \"text\": \" open coding may be apply in vary degree of detail . the code can be link to a line , a sentence , a paragraph or wholesome text ( protocol , case , etc . ) . the application of the alternative depend on the research question , on the relevant datum , personal style of analyst and the stage of research . however , while cod , the main aim of coding should be in sight i.e. to break down and understand the text and develop category to be put in order in the course of time . \", \"tokencount\": 99, \"category\": \"Data analysis\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 381, \"title\": \"Open coding\", \"text\": \" the result of open coding should be a list characterise code and category attach to the text and support by code note that be produce to explain the content of code . these note could be strike observation and thought that be relevant to the development of theory . \", \"tokencount\": 49, \"category\": \"Data analysis\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 381, \"title\": \"Open coding\", \"text\": \" although code be exclusive to the research material and the style of the analyst , it be suggest researcher should address the text with the follow question : \\n  what ? - identify the underlying issue and the phenomenon \\n  who ? - identify the actor involve and the role they play . \\n  how ? - identify the aspect of phenomenon \\n  when ? how long ? where ? - time , course and location \\n  how much ? how long ? - identify the intensity \\n  why ? - identify the reason attach to the phenomenon \\n  what for ? - identify intention or purpose \\n  by which ? - strategy and tactic to achieve the goal \", \"tokencount\": 119, \"category\": \"Data analysis\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 382, \"title\": \"Connect (computer system)\", \"text\": \" connect be a new social network analysis software datum mining computer system develop by hmrc ( uk ) that cross - reference business 's and people 's tax record with other database to establish fraudulent or undisclosed ( misdirected ) activity . \", \"tokencount\": 42, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 390, \"title\": \"Computational X\", \"text\": \" computational x be the set of field of study that have emerge from the application of informatic and big datum to specific discipline . example include computational biology , computational neuroscience , computational physics , and computational linguistic . \", \"tokencount\": 39, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 391, \"title\": \"Negative testing\", \"text\": \" negative testing be a method of test an application or system that ensure that the plot of the application be accord to the requirement and can handle the unwanted input and user behavior . invalid datum be insert to compare the output against the give input . negative testing be also know as failure testing or error path testing . when perform negative testing exception be expect . this show that the application be able to handle improper user behavior . user input value that do not work in the system to test its ability to handle incorrect value or system failure . \", \"tokencount\": 103, \"category\": \"Data analysis\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 395, \"title\": \"Instance selection\", \"text\": \" instance selection ( or dataset reduction , or dataset condensation ) be an important datum pre - processing step that can be apply in many machine learning ( or datum mining ) task . s. garc\\u00eda , j. luengo , and f. herrera , data preprocess in datum mining . springer , 2015 . approach for instance selection can be apply for reduce the original dataset to a manageable volume , lead to a reduction of the computational resource that be necessary for perform the learning process . algorithms of instance selection can also be apply for remove noisy instance , before apply learn algorithm . this step can improve the accuracy in classification problem . \", \"tokencount\": 116, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 395, \"title\": \"Instance selection\", \"text\": \" algorithm for instance selection should identify a subset of the total available datum to achieve the original purpose of the datum mining ( or machine learning ) application as if the whole datum have be use . consider this , the optimal outcome of be would be the minimum datum subset that can accomplish the same task with no performance loss , in comparison with the performance achieve when the task be perform use the whole available datum . therefore , every instance selection strategy should deal with a trade - off between the reduction rate of the dataset and the classification quality . \", \"tokencount\": 104, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 396, \"title\": \"Social profiling\", \"text\": \" social profiling be the process of construct a user 's profile use his or her social datum . in general , profile refer to the data science process of generate a person 's profile with computerized algorithm and technology . there be many medium and platform for share these information with the help of the increase number of successful social network , include but not limit to linkedin , google+ , facebook and twitter etc . \", \"tokencount\": 76, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 397, \"title\": \"Outline of machine learning\", \"text\": \" the follow outline be provide as an overview of and topical guide to machine learning . machine learning be a subfield of soft computing within computer science that evolve from the study of pattern recognition and computational learning theory in artificial intelligence.http://www.britannica.com/ebchecked/topic/1116194/machine-learning   in 1959 , arthur samuel define machine learn as a \\\" field of study that give computer the ability to learn without be explicitly program \\\" . machine learning explore the study and construction of algorithm that can learn from and make prediction on datum . such algorithm operate by build a model from an example training set of input observation in order to make data - drive prediction or decision express as output , rather than follow strictly static program instruction . \", \"tokencount\": 126, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 404, \"title\": \"Machine learning in bioinformatics\", \"text\": \" machine learning , a subfield of computer science involve the development of algorithm that learn how to make prediction base on datum , have a number of emerge application in the field of bioinformatic . bioinformatic deal with computational and mathematical approach for understanding and process biological datum . \", \"tokencount\": 49, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 404, \"title\": \"Machine learning in bioinformatics\", \"text\": \" prior to the emergence of machine learn algorithm , bioinformatic algorithm have to be explicitly program by hand which , for problem such as protein structure prediction , prove extremely difficult . machine learning technique such as deep learning enable the algorithm to make use of automatic feature learning which mean that base on the dataset alone , the algorithm can learn how to combine multiple feature of the input datum into a more abstract set of feature from which to conduct further learning . this multi - layer approach to learn pattern in the input data allow such system to make quite complex prediction when train on large dataset . in recent year , the size and number of available biological dataset have skyrocket , enable bioinformatic researcher to make use of these machine learning system . machine learning have be apply to six main subfield of bioinformatic : genomic , proteomic , microarray , system biology , evolution , and text mining . \", \"tokencount\": 165, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 406, \"title\": \"Arthur Zimek\", \"text\": \" arthur zimek be a professor in datum mining , data science and machine learn at the university of southern denmark in odense , denmark . \", \"tokencount\": 25, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 406, \"title\": \"Arthur Zimek\", \"text\": \" He graduate from the ludwig maximilian university of munich in munich , germany , where he work with prof . hans - peter kriegel . His dissertation on \\\" correlation clustering \\\" be award the \\\" sigkdd doctoral dissertation award 2009 runner - up \\\" by the association for computing machinery . \", \"tokencount\": 52, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 406, \"title\": \"Arthur Zimek\", \"text\": \" He be well knowne.g .   for his work on outlier detection , density - base clustering , correlation clustering , and the curse of dimensionality . \", \"tokencount\": 27, \"category\": \"Machine learning researchers\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 406, \"title\": \"Arthur Zimek\", \"text\": \" He be one of the founder and core developer of the open - source elki datum mining framework . \", \"tokencount\": 19, \"category\": \"Machine learning researchers\", \"lineid\": 3, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 408, \"title\": \"International Journal of Data Warehousing and Mining\", \"text\": \" the international journal of data warehousing and mining ( ijdwm ) . be a quarterly peer - review academic journal cover datum warehousing and datum mining . It be establish in 2005 and be publish by igi global . the editor - in - chief be david taniar ( monash university , australia ) . \", \"tokencount\": 55, \"category\": \"Data mining\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 410, \"title\": \"Hyperparameter optimization\", \"text\": \" in machine learning , hyperparameter optimization or tuning be the problem of choose a set of optimal hyperparameter for a learn algorithm . a hyperparameter be a parameter whose value be use to control the learning process . by contrast , the value of other parameter ( typically node weight ) be learn . \\n the same kind of machine learning model can require different constraint , weight or learning rate to generalize different datum pattern . these measure be call hyperparameter , and have to be tune so that the model can optimally solve the machine learning problem . hyperparameter optimization find a tuple of hyperparameter that yield an optimal model which minimize a predefined loss function on give independent datum .   the objective function take a tuple of hyperparameter and return the associated loss . cross - validation be often use to estimate this generalization performance . \", \"tokencount\": 150, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 417, \"title\": \"Seeq Corporation\", \"text\": \" seeq corporation be a software company , seeq raise $ 6 million , look to help manufacturer mine industrial datum . geekwire . retrieved 2017 - 10 - 18 . found in 2013   and headquarter in seattle , washington , united states , that provide software with advanced analytic capability to the industrial process manufacturing sector include pharmaceutical , creating value from data assets . pharmaceutical manufacturing . retrieved 2017 - 10 - 18 . oil and gas , remotely analyze gas processing datum . isa.org . retrieved 2017 - 10 - 18 . mining and mineral , pulp and paper , energy and utility , harness big datum to achieve big gain . control . retrieved 2017 - 10 - 18 . iiot , iiot optimize evaporative cooling . control engineering . retrieved 2017 - 10 - 18 . and chemical industry among other . seeq 's browser - base software be design specifically for use with time series data\\u201cseeq \\\" out new manufacturing intelligence methods . automationworld . retrieved 2017 - 10 - 18 . which be most often aggregate in datum historian such as osisoft 's pi system , inductive automation 's ignition system and other similar datum historian such as emerson 's ovation and deltav , ge proficy , honeywell 's uniformance phd , wonderware , and aspentech ip.21 , as well as many other . leverage big datum to streamline plant operation . control . retrieved 2017 - 10 - 18 . \", \"tokencount\": 248, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 417, \"title\": \"Seeq Corporation\", \"text\": \" seeq 's multiple application allow organization to analyze their datum to improve business outcome . workbench , one of seeq 's application , include data visualization , datum modeling , and interactive tool for diagnostic , monitoring , predictive and descriptive analyticsleveraging data analytics innovations to improve process outcomes . biopharm international . retrieved 2017 - 10 - 18 . It also include google - like search , knowledge - capture and collaboration tool . seeq organizer be use to create document that assemble analysis and visualization into report , presentation , and meeting agenda . organizer document be dynamic because they tie directly to the underlie datum , and be \\\" time relative \\\" so they can be define by any batch , shift , day , etc . seeq runtime perform continuous datum cleansing , boundary management , and stream calculation on historian datum . the runtime , which be access through either seeq workbench or the seeq rest api , run autonomously and may be integrate with exist alarm system or dashboard solution . \", \"tokencount\": 177, \"category\": \"Data analysis software\", \"lineid\": 1, \"term\": \"datum\", \"count\": 2.8638460851400347}, {\"docid\": 417, \"title\": \"Seeq Corporation\", \"text\": \" seeq can be set up and run on a dedicated server , server cluster , or virtual machine in as little time as an hour . on - premise installation on the same network as a plant or enterprise historian , or on the cloud ( microsoft azure , amazon web services , etc . ) , or on a mixed environment of on - premise and cloud resource be support . seeq be extensible through datum export , data integration , and a rest api for create custom template and module . datum export option include microsoft excel and powerpoint , and any odata client ( tableau , microsoft powerbi , etc . ) . datum integration with osisoft vision be support , and the rest api have sdks for programming in c # , python , matlab , and java . seeq do not copy or duplicate datum from the source of record . instead , datum be access via a connector retrieve datum on the fly base on user action . all seeq document such as workbook , worksheet , topic , and search definition be store by seeq for easy user access and sharing . \", \"tokencount\": 199, \"category\": \"Data analysis software\", \"lineid\": 2, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 418, \"title\": \"Algorithmic bias\", \"text\": \" algorithmic bias describe systematic and repeatable error in a computer system that create unfair outcome , such as privilege one arbitrary group of user over other . bias can emerge due to many factor , include but not limit to the design of the algorithm itself , unintended or unanticipated use or decision relate to the way data be cod , collect , select or use to train the algorithm . algorithmic bias be find across platform , include but not limit to search engine result and social medium platform , and can have impact range from inadvertent privacy violation to reinforce social bias of race , gender , sexuality , and ethnicity . the study of algorithmic bias be most concerned with algorithm that reflect \\\" systematic and unfair \\\" discrimination . this bias have only recently be address in legal framework , such as the 2018 european union 's general data protection regulation . \", \"tokencount\": 156, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 418, \"title\": \"Algorithmic bias\", \"text\": \" as algorithm expand their ability to organize society , politic , institution , and behavior , sociologist have become concerned with the way in which unanticipated output and manipulation of datum can impact the physical world . because algorithm be often consider to be neutral and unbiased , they can inaccurately project great authority than human expertise , and in some case , reliance on algorithm can displace human responsibility for their outcome . bias can enter into algorithmic system as a result of pre - exist cultural , social , or institutional expectation ; because of technical limitation of their design ; or by be use in unanticipated context or by audience who be not consider in the software 's initial design . \", \"tokencount\": 124, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 418, \"title\": \"Algorithmic bias\", \"text\": \" algorithmic bias have be cite in case range from election outcome to the spread of online hate speech . problem in understanding , research , and discover algorithmic bias stem from the proprietary nature of algorithm , which be typically treat as trade secret . even when full transparency be provide , the complexity of certain algorithm pose a barrier to understand their functioning . furthermore , algorithm may change , or respond to input or output in way that can not be anticipate or easily reproduce for analysis . in many case , even within a single website or application , there be no single \\\" algorithm \\\" to examine , but a network of many interrelated program and datum input , even between user of the same service . \", \"tokencount\": 131, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 419, \"title\": \"Automated machine learning\", \"text\": \" automated machine learning ( automl ) be the process of automate the end - to - end process of apply machine learn to real - world problem . in a typical machine learning application , practitioner must apply the appropriate datum pre - processing , feature engineering , feature extraction , and feature selection method that make the dataset amenable for machine learning . follow those preprocess step , practitioner must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model . as many of these step be often beyond the ability of non - expert , automl be propose as an artificial intelligence - base solution to the ever - grow challenge of apply machine learning . automate the end - to - end process of apply machine learning offer the advantage of produce simple solution , fast creation of those solution , and model that often outperform model that be design by hand . \", \"tokencount\": 164, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 427, \"title\": \"Paul Viola\", \"text\": \" paul viola be a computer vision researcher , former mit professor , and vice president of science for amazon air . He be good \\n know for his seminal work in facial recognition and machine learning .    He be \\n the co - inventor of the viola \\u2013 jones object detection framework along with michael jones . He win the marr prize in 2003 and the helmholtz prize from the international conference on computer vision in 2013 . \\n He \\n be the holder of at least 57 patent in the area of advanced machine learning , web search , \\n datum mining , and image processing . \\n He be the author of more than 50 academic \\n research paper with over 56,000 citation . \\n \", \"tokencount\": 128, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 434, \"title\": \"Appen (company)\", \"text\": \" appen limited ( formerly know as appen butler hill ) be a publicly trade company list on the australian securities exchange ( asx ) under the code apx . \", \"tokencount\": 29, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 434, \"title\": \"Appen (company)\", \"text\": \" appen provide or improve datum use for the development of machine learning and artificial intelligence product . datum type include speech and natural language datum , image and video datum , text and alphanumeric datum and relevance datum to improve search and social medium engine . appen 's customer use machine learn for a variety of use case include automatic speech recognition ( asr ) , computer vision , increase conversion in ecommerce , deliver more meaningful and personalize advertising , enhance social medium feed or improve customer service capability with tool like chatbot and virtual assistant . \", \"tokencount\": 98, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 3.5286813832108277}, {\"docid\": 434, \"title\": \"Appen (company)\", \"text\": \" for machine to demonstrate artificial intelligence , they need to be program with human - quality training datum that help them learn .   appen use crowdsourcing to collect and improve datum and have access to a skilled crowd of over than 1,000,000 part - time contractor who collect , annotate , evaluate , label , rate , test , translate and transcribe speech , image , text and video datum to turn it into effective machine learn training datum for a variety of use case . \", \"tokencount\": 87, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 435, \"title\": \"Geoffrey J. Gordon\", \"text\": \" geoffrey j. gordon be a professor at the machine learning department at carnegie mellon university in pittsburgh and director of research at the microsoft montr\\u00e9al lab . microsoft appoint carnegie mellon professor to head expand montreal ai research lab , itbusiness.ca , 2018 - 01 - 24leader in davos acknowledge ai \\u2019s potential for good , but point to unanswered question , justin trudeau twitter about gordons appointment from wef , itbusiness.ca . 2018 - 01 - 24.here 's why canada can win the ai race , forbes , 2018 - 03 - 13canadian tech sector thrives , but struggle to keep Its talent , wall street journal , 2018 - 02 - 08.microsoft announce expansion of montreal ai research lab , windowscentral , 2018 - 01 - 24 . He be know for his research in statistical relational learning ( a subdiscipline of artificial intelligence and machine learning ) and on anytime dynamic variant of the a * search algorithm . likhachev , maxim ; gordon , geoff ; thrun , sebastian . \\\" ara * : anytime a * search with provable bound on sub - optimality \\\" . in s. thrun , l. saul , and b. sch\\u00f6lkopf , editor , proceedings of conference on neural information processing systems ( nips ) , cambridge , ma , 2003 . mit press . His research interest include multi - agent planning , reinforcement learning , decision - theoretic planning , statistical model of difficult datum ( e.g. map , video , text ) , computational learning theory , and game theory . \", \"tokencount\": 264, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 435, \"title\": \"Geoffrey J. Gordon\", \"text\": \" gordon receive a b.a. in computer science from cornell university in 1991 , and a phd at carnegie mellon in 1999 . \", \"tokencount\": 22, \"category\": \"Machine learning researchers\", \"lineid\": 1, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 438, \"title\": \"Tidyverse\", \"text\": \" the tidyverse be a collection of r package introduce by hadley wickham and his team that \\\" share an underlying design philosophy , grammar , and data structure \\\" of tidy datum . the core package be ggplot2 , dplyr , tidyr , readr , purrr , tibble , stringr , and forcat , which provide functionality to model , transform , and visualize datum . as of november 2018 , the tidyverse package and some of its individual package make up 5 out of the top 10 most download r package , and be the subject of multiple book and paper . \", \"tokencount\": 103, \"category\": \"Data analysis software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 446, \"title\": \"Learning curve (machine learning)\", \"text\": \" a learn curve show the validation and training score of an estimator for vary number of training sample . It be a tool to find out how much a machine learn model benefit from add more training datum and whether the estimator suffer more from a variance error or a bias error . if both the validation score and the training score converge to a value that be too low with increase size of the training set , it will not benefit much from more training datum . \", \"tokencount\": 88, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 446, \"title\": \"Learning curve (machine learning)\", \"text\": \" the machine learn curve be useful for many purpose include compare different algorithm ,   choose model parameter during design , adjust optimization to improve convergence , and determine the amount of datum use for training . \", \"tokencount\": 37, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 446, \"title\": \"Learning curve (machine learning)\", \"text\": \" in the machine learn domain , there be two connotation of learning curve differ in the x - axis of the curve , with experience of the model graph either as the number of training example use for learning or the number of iteration use in train the model . \", \"tokencount\": 50, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 451, \"title\": \"Weak supervision\", \"text\": \" weak supervision be a branch of machine learn where noisy , limited , or imprecise source be use to provide supervision signal for label large amount of training datum in a supervised learning setting . this approach alleviate the burden of obtain hand - label data set , which can be costly or impractical . instead , inexpensive weak label be employ with the understanding that they be imperfect , but can nonetheless be use to create a strong predictive model . \", \"tokencount\": 82, \"category\": \"Machine learning researchers\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}, {\"docid\": 452, \"title\": \"Federated learning\", \"text\": \" federated learning designate a set of technique strive to simultaneously train a single machine learn algorithm across multiple decentralize server hold local datum sample , without exchange their datum sample . this approach stand in contrast to traditional centralized machine learning technique where all datum sample be uploaded to one server , as well as to more classical decentralized approach which assume that local datum sample be identically distribute . \", \"tokencount\": 70, \"category\": \"Machine learning\", \"lineid\": 0, \"term\": \"datum\", \"count\": 2.199010787069242}, {\"docid\": 452, \"title\": \"Federated learning\", \"text\": \" federated learning enable multiple actor to build a common , robust machine learning model without share datum sample , thus address critical issue such as data privacy , datum security , data access right and access to heterogeneous datum . Its application be spread over a number of industry include defense , telecommunications , iot , or pharmaceutic . \", \"tokencount\": 59, \"category\": \"Machine learning\", \"lineid\": 1, \"term\": \"datum\", \"count\": 1.5341754889984494}, {\"docid\": 452, \"title\": \"Federated learning\", \"text\": \"   definition \\n federated learning aim at train a machine learn algorithm , for instance deep neural network , on multiple local dataset contain in local node without exchange datum sample . the general principle consist in train local model on local datum sample and exchange parameter ( e.g. the weight of a deep neural network ) between these local model at some frequency to generate a global model . \", \"tokencount\": 70, \"category\": \"Machine learning\", \"lineid\": 2, \"term\": \"datum\", \"count\": 0.8693401909276565}, {\"docid\": 452, \"title\": \"Federated learning\", \"text\": \" federated learn algorithm may use a central server that orchestrate the different step of the algorithm and act as a reference clock , or they may be peer - to - peer , where no such central server exist . in the non peer - to - peer case , a federated learning process can be break down in multiple round , each consist of 4 general step . \", \"tokencount\": 69, \"category\": \"Machine learning\", \"lineid\": 3, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 452, \"title\": \"Federated learning\", \"text\": \" the main difference between federated learning and distribute learning lie in the assumption make on the property of the local datasetsfederated optimization : distributed optimization beyond the datacenter , jakub konecny , h. brendan mcmahan , daniel ramage , 2015 , as distribute learn originally aim at parallelize computing power where federated learning originally aim at training on heterogeneous dataset . while distribute learning also aim at train a single model on multiple server , a common underlying assumption be that the local dataset be identically distribute and roughly have the same size . none of these hypothesis be make for federated learning ; instead , the dataset be typically heterogeneous and their size may span several order of magnitude . \", \"tokencount\": 121, \"category\": \"Machine learning\", \"lineid\": 4, \"term\": \"datum\", \"count\": -0.460330405213929}, {\"docid\": 453, \"title\": \"Weaviate\", \"text\": \" weaviate be an open source knowledge graph base on a vector storage mechanism call the contextionary . It allow a user to search for context or keyword in a dataset rather than fix keyword alone . It provide a graphql interface to query the knowledge graph and an http web interface to add the datum via an ontology schema . \", \"tokencount\": 60, \"category\": \"Data mining and machine learning software\", \"lineid\": 0, \"term\": \"datum\", \"count\": 0.2045048928568637}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.FacetChart(...)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "general_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}